{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM8VOJONJ9DrMkuoEivJ5f8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pedro162/machine_learn/blob/main/machine_learn_redes_neurais.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/driver\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q7qhLNfpUGj6",
        "outputId": "0c6376c6-e7d2-49d5-93e6-a1c3bb532f69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/driver\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m8MYwzvOTh4D"
      },
      "outputs": [],
      "source": [
        "from sklearn.neural_network import MLPClassifier"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Base credit data"
      ],
      "metadata": {
        "id": "KwZfytsOUn7C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "with open(\"/content/driver/MyDrive/credit.pkl\", 'rb') as f:\n",
        "  X_credit_treinamento, X_credit_teste, y_credit_treinamento, y_credit_teste = pickle.load(f)"
      ],
      "metadata": {
        "id": "WHMT4LZGUqYd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_credit_treinamento.shape, y_credit_treinamento.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZU7rpB7nVGM9",
        "outputId": "b1dd7f1d-62af-4812-b83f-d9461e6dea9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1500, 3), (1500,))"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_credit_teste.shape, y_credit_teste.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QBhMJuy0VKsO",
        "outputId": "6eee4921-d681-4cd1-d4de-fcd76a980588"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((500, 3), (500,))"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(3+1) / 2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "va-zg17OYW5q",
        "outputId": "8bd5b63a-5d71-4632-e7bc-4fa1ad53e736"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.0"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rede_neural_credit = MLPClassifier(max_iter=15000, verbose=True, tol=0.0000100, solver='adam', activation='relu', hidden_layer_sizes=(2,2))\n",
        "rede_neural_credit.fit(X_credit_treinamento, y_credit_treinamento)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6nmdBeYMV6G_",
        "outputId": "11d1c5c8-b96d-490a-b2ff-c11cb931b48c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 0.81255092\n",
            "Iteration 2, loss = 0.80211749\n",
            "Iteration 3, loss = 0.79240821\n",
            "Iteration 4, loss = 0.78291995\n",
            "Iteration 5, loss = 0.77450238\n",
            "Iteration 6, loss = 0.76659200\n",
            "Iteration 7, loss = 0.75898132\n",
            "Iteration 8, loss = 0.75215241\n",
            "Iteration 9, loss = 0.74554956\n",
            "Iteration 10, loss = 0.73945120\n",
            "Iteration 11, loss = 0.73371910\n",
            "Iteration 12, loss = 0.72831047\n",
            "Iteration 13, loss = 0.72297054\n",
            "Iteration 14, loss = 0.71797538\n",
            "Iteration 15, loss = 0.71318511\n",
            "Iteration 16, loss = 0.70845636\n",
            "Iteration 17, loss = 0.70412444\n",
            "Iteration 18, loss = 0.70024996\n",
            "Iteration 19, loss = 0.69654064\n",
            "Iteration 20, loss = 0.69310755\n",
            "Iteration 21, loss = 0.68975464\n",
            "Iteration 22, loss = 0.68654893\n",
            "Iteration 23, loss = 0.68338020\n",
            "Iteration 24, loss = 0.68027463\n",
            "Iteration 25, loss = 0.67725673\n",
            "Iteration 26, loss = 0.67427944\n",
            "Iteration 27, loss = 0.67130635\n",
            "Iteration 28, loss = 0.66838337\n",
            "Iteration 29, loss = 0.66546912\n",
            "Iteration 30, loss = 0.66263636\n",
            "Iteration 31, loss = 0.65971922\n",
            "Iteration 32, loss = 0.65688235\n",
            "Iteration 33, loss = 0.65402852\n",
            "Iteration 34, loss = 0.65118218\n",
            "Iteration 35, loss = 0.64837724\n",
            "Iteration 36, loss = 0.64552070\n",
            "Iteration 37, loss = 0.64267832\n",
            "Iteration 38, loss = 0.63980708\n",
            "Iteration 39, loss = 0.63692361\n",
            "Iteration 40, loss = 0.63404069\n",
            "Iteration 41, loss = 0.63116750\n",
            "Iteration 42, loss = 0.62827931\n",
            "Iteration 43, loss = 0.62539110\n",
            "Iteration 44, loss = 0.62251887\n",
            "Iteration 45, loss = 0.61961497\n",
            "Iteration 46, loss = 0.61674500\n",
            "Iteration 47, loss = 0.61385431\n",
            "Iteration 48, loss = 0.61095417\n",
            "Iteration 49, loss = 0.60809248\n",
            "Iteration 50, loss = 0.60527104\n",
            "Iteration 51, loss = 0.60244254\n",
            "Iteration 52, loss = 0.59959402\n",
            "Iteration 53, loss = 0.59679254\n",
            "Iteration 54, loss = 0.59403565\n",
            "Iteration 55, loss = 0.59131315\n",
            "Iteration 56, loss = 0.58859904\n",
            "Iteration 57, loss = 0.58589215\n",
            "Iteration 58, loss = 0.58317658\n",
            "Iteration 59, loss = 0.58050506\n",
            "Iteration 60, loss = 0.57775553\n",
            "Iteration 61, loss = 0.57508421\n",
            "Iteration 62, loss = 0.57236333\n",
            "Iteration 63, loss = 0.56971592\n",
            "Iteration 64, loss = 0.56706765\n",
            "Iteration 65, loss = 0.56442273\n",
            "Iteration 66, loss = 0.56173070\n",
            "Iteration 67, loss = 0.55906244\n",
            "Iteration 68, loss = 0.55638073\n",
            "Iteration 69, loss = 0.55371209\n",
            "Iteration 70, loss = 0.55107349\n",
            "Iteration 71, loss = 0.54844830\n",
            "Iteration 72, loss = 0.54582581\n",
            "Iteration 73, loss = 0.54321699\n",
            "Iteration 74, loss = 0.54060377\n",
            "Iteration 75, loss = 0.53803434\n",
            "Iteration 76, loss = 0.53543779\n",
            "Iteration 77, loss = 0.53286370\n",
            "Iteration 78, loss = 0.53032632\n",
            "Iteration 79, loss = 0.52776155\n",
            "Iteration 80, loss = 0.52525673\n",
            "Iteration 81, loss = 0.52268694\n",
            "Iteration 82, loss = 0.52017221\n",
            "Iteration 83, loss = 0.51764535\n",
            "Iteration 84, loss = 0.51513495\n",
            "Iteration 85, loss = 0.51261663\n",
            "Iteration 86, loss = 0.51010879\n",
            "Iteration 87, loss = 0.50763284\n",
            "Iteration 88, loss = 0.50512770\n",
            "Iteration 89, loss = 0.50268673\n",
            "Iteration 90, loss = 0.50020577\n",
            "Iteration 91, loss = 0.49779084\n",
            "Iteration 92, loss = 0.49534539\n",
            "Iteration 93, loss = 0.49290890\n",
            "Iteration 94, loss = 0.49055550\n",
            "Iteration 95, loss = 0.48813591\n",
            "Iteration 96, loss = 0.48578531\n",
            "Iteration 97, loss = 0.48344689\n",
            "Iteration 98, loss = 0.48110719\n",
            "Iteration 99, loss = 0.47876961\n",
            "Iteration 100, loss = 0.47642550\n",
            "Iteration 101, loss = 0.47411892\n",
            "Iteration 102, loss = 0.47181709\n",
            "Iteration 103, loss = 0.46954619\n",
            "Iteration 104, loss = 0.46727201\n",
            "Iteration 105, loss = 0.46498332\n",
            "Iteration 106, loss = 0.46276019\n",
            "Iteration 107, loss = 0.46047946\n",
            "Iteration 108, loss = 0.45824286\n",
            "Iteration 109, loss = 0.45600168\n",
            "Iteration 110, loss = 0.45377213\n",
            "Iteration 111, loss = 0.45157498\n",
            "Iteration 112, loss = 0.44933827\n",
            "Iteration 113, loss = 0.44714545\n",
            "Iteration 114, loss = 0.44496280\n",
            "Iteration 115, loss = 0.44273498\n",
            "Iteration 116, loss = 0.44052720\n",
            "Iteration 117, loss = 0.43835403\n",
            "Iteration 118, loss = 0.43617040\n",
            "Iteration 119, loss = 0.43402711\n",
            "Iteration 120, loss = 0.43183200\n",
            "Iteration 121, loss = 0.42967379\n",
            "Iteration 122, loss = 0.42750966\n",
            "Iteration 123, loss = 0.42536134\n",
            "Iteration 124, loss = 0.42323191\n",
            "Iteration 125, loss = 0.42112265\n",
            "Iteration 126, loss = 0.41902847\n",
            "Iteration 127, loss = 0.41687866\n",
            "Iteration 128, loss = 0.41476931\n",
            "Iteration 129, loss = 0.41250776\n",
            "Iteration 130, loss = 0.41027497\n",
            "Iteration 131, loss = 0.40806048\n",
            "Iteration 132, loss = 0.40584383\n",
            "Iteration 133, loss = 0.40368436\n",
            "Iteration 134, loss = 0.40145651\n",
            "Iteration 135, loss = 0.39921444\n",
            "Iteration 136, loss = 0.39697578\n",
            "Iteration 137, loss = 0.39471628\n",
            "Iteration 138, loss = 0.39241659\n",
            "Iteration 139, loss = 0.39022993\n",
            "Iteration 140, loss = 0.38800234\n",
            "Iteration 141, loss = 0.38581995\n",
            "Iteration 142, loss = 0.38369965\n",
            "Iteration 143, loss = 0.38146425\n",
            "Iteration 144, loss = 0.37930795\n",
            "Iteration 145, loss = 0.37709169\n",
            "Iteration 146, loss = 0.37495564\n",
            "Iteration 147, loss = 0.37289957\n",
            "Iteration 148, loss = 0.37082743\n",
            "Iteration 149, loss = 0.36886679\n",
            "Iteration 150, loss = 0.36684492\n",
            "Iteration 151, loss = 0.36492437\n",
            "Iteration 152, loss = 0.36298061\n",
            "Iteration 153, loss = 0.36107062\n",
            "Iteration 154, loss = 0.35914817\n",
            "Iteration 155, loss = 0.35728292\n",
            "Iteration 156, loss = 0.35539484\n",
            "Iteration 157, loss = 0.35350484\n",
            "Iteration 158, loss = 0.35169790\n",
            "Iteration 159, loss = 0.34983552\n",
            "Iteration 160, loss = 0.34802092\n",
            "Iteration 161, loss = 0.34620298\n",
            "Iteration 162, loss = 0.34437645\n",
            "Iteration 163, loss = 0.34262149\n",
            "Iteration 164, loss = 0.34080469\n",
            "Iteration 165, loss = 0.33903644\n",
            "Iteration 166, loss = 0.33726476\n",
            "Iteration 167, loss = 0.33552838\n",
            "Iteration 168, loss = 0.33379829\n",
            "Iteration 169, loss = 0.33208940\n",
            "Iteration 170, loss = 0.33036837\n",
            "Iteration 171, loss = 0.32864735\n",
            "Iteration 172, loss = 0.32697506\n",
            "Iteration 173, loss = 0.32527701\n",
            "Iteration 174, loss = 0.32358331\n",
            "Iteration 175, loss = 0.32193024\n",
            "Iteration 176, loss = 0.32032064\n",
            "Iteration 177, loss = 0.31871697\n",
            "Iteration 178, loss = 0.31706198\n",
            "Iteration 179, loss = 0.31544077\n",
            "Iteration 180, loss = 0.31385014\n",
            "Iteration 181, loss = 0.31227292\n",
            "Iteration 182, loss = 0.31071546\n",
            "Iteration 183, loss = 0.30912985\n",
            "Iteration 184, loss = 0.30759336\n",
            "Iteration 185, loss = 0.30608041\n",
            "Iteration 186, loss = 0.30457923\n",
            "Iteration 187, loss = 0.30303482\n",
            "Iteration 188, loss = 0.30155574\n",
            "Iteration 189, loss = 0.30006297\n",
            "Iteration 190, loss = 0.29858874\n",
            "Iteration 191, loss = 0.29710601\n",
            "Iteration 192, loss = 0.29564671\n",
            "Iteration 193, loss = 0.29418996\n",
            "Iteration 194, loss = 0.29281655\n",
            "Iteration 195, loss = 0.29131477\n",
            "Iteration 196, loss = 0.28989720\n",
            "Iteration 197, loss = 0.28847423\n",
            "Iteration 198, loss = 0.28704884\n",
            "Iteration 199, loss = 0.28562843\n",
            "Iteration 200, loss = 0.28430355\n",
            "Iteration 201, loss = 0.28285060\n",
            "Iteration 202, loss = 0.28149526\n",
            "Iteration 203, loss = 0.28010260\n",
            "Iteration 204, loss = 0.27872368\n",
            "Iteration 205, loss = 0.27737179\n",
            "Iteration 206, loss = 0.27600199\n",
            "Iteration 207, loss = 0.27464938\n",
            "Iteration 208, loss = 0.27332983\n",
            "Iteration 209, loss = 0.27201452\n",
            "Iteration 210, loss = 0.27069855\n",
            "Iteration 211, loss = 0.26935500\n",
            "Iteration 212, loss = 0.26806467\n",
            "Iteration 213, loss = 0.26682698\n",
            "Iteration 214, loss = 0.26552824\n",
            "Iteration 215, loss = 0.26428211\n",
            "Iteration 216, loss = 0.26297859\n",
            "Iteration 217, loss = 0.26170106\n",
            "Iteration 218, loss = 0.26045001\n",
            "Iteration 219, loss = 0.25921603\n",
            "Iteration 220, loss = 0.25799771\n",
            "Iteration 221, loss = 0.25676146\n",
            "Iteration 222, loss = 0.25552177\n",
            "Iteration 223, loss = 0.25430743\n",
            "Iteration 224, loss = 0.25308163\n",
            "Iteration 225, loss = 0.25191136\n",
            "Iteration 226, loss = 0.25071835\n",
            "Iteration 227, loss = 0.24954818\n",
            "Iteration 228, loss = 0.24836282\n",
            "Iteration 229, loss = 0.24719399\n",
            "Iteration 230, loss = 0.24602544\n",
            "Iteration 231, loss = 0.24486487\n",
            "Iteration 232, loss = 0.24370490\n",
            "Iteration 233, loss = 0.24256730\n",
            "Iteration 234, loss = 0.24143391\n",
            "Iteration 235, loss = 0.24030028\n",
            "Iteration 236, loss = 0.23916817\n",
            "Iteration 237, loss = 0.23808139\n",
            "Iteration 238, loss = 0.23701722\n",
            "Iteration 239, loss = 0.23591116\n",
            "Iteration 240, loss = 0.23479199\n",
            "Iteration 241, loss = 0.23370454\n",
            "Iteration 242, loss = 0.23262734\n",
            "Iteration 243, loss = 0.23151815\n",
            "Iteration 244, loss = 0.23047729\n",
            "Iteration 245, loss = 0.22952009\n",
            "Iteration 246, loss = 0.22837958\n",
            "Iteration 247, loss = 0.22729295\n",
            "Iteration 248, loss = 0.22624951\n",
            "Iteration 249, loss = 0.22519282\n",
            "Iteration 250, loss = 0.22418992\n",
            "Iteration 251, loss = 0.22314890\n",
            "Iteration 252, loss = 0.22214633\n",
            "Iteration 253, loss = 0.22112471\n",
            "Iteration 254, loss = 0.22011341\n",
            "Iteration 255, loss = 0.21911046\n",
            "Iteration 256, loss = 0.21811715\n",
            "Iteration 257, loss = 0.21714674\n",
            "Iteration 258, loss = 0.21615603\n",
            "Iteration 259, loss = 0.21518047\n",
            "Iteration 260, loss = 0.21421949\n",
            "Iteration 261, loss = 0.21328025\n",
            "Iteration 262, loss = 0.21232951\n",
            "Iteration 263, loss = 0.21137556\n",
            "Iteration 264, loss = 0.21040482\n",
            "Iteration 265, loss = 0.20945665\n",
            "Iteration 266, loss = 0.20852672\n",
            "Iteration 267, loss = 0.20761567\n",
            "Iteration 268, loss = 0.20669319\n",
            "Iteration 269, loss = 0.20575590\n",
            "Iteration 270, loss = 0.20487521\n",
            "Iteration 271, loss = 0.20399198\n",
            "Iteration 272, loss = 0.20302681\n",
            "Iteration 273, loss = 0.20213738\n",
            "Iteration 274, loss = 0.20127739\n",
            "Iteration 275, loss = 0.20037291\n",
            "Iteration 276, loss = 0.19947322\n",
            "Iteration 277, loss = 0.19859827\n",
            "Iteration 278, loss = 0.19773277\n",
            "Iteration 279, loss = 0.19691499\n",
            "Iteration 280, loss = 0.19598431\n",
            "Iteration 281, loss = 0.19514215\n",
            "Iteration 282, loss = 0.19426976\n",
            "Iteration 283, loss = 0.19341119\n",
            "Iteration 284, loss = 0.19257236\n",
            "Iteration 285, loss = 0.19171809\n",
            "Iteration 286, loss = 0.19092843\n",
            "Iteration 287, loss = 0.19009712\n",
            "Iteration 288, loss = 0.18927929\n",
            "Iteration 289, loss = 0.18843340\n",
            "Iteration 290, loss = 0.18758104\n",
            "Iteration 291, loss = 0.18679518\n",
            "Iteration 292, loss = 0.18597375\n",
            "Iteration 293, loss = 0.18517455\n",
            "Iteration 294, loss = 0.18436525\n",
            "Iteration 295, loss = 0.18358489\n",
            "Iteration 296, loss = 0.18280891\n",
            "Iteration 297, loss = 0.18202867\n",
            "Iteration 298, loss = 0.18124551\n",
            "Iteration 299, loss = 0.18045976\n",
            "Iteration 300, loss = 0.17968119\n",
            "Iteration 301, loss = 0.17892218\n",
            "Iteration 302, loss = 0.17817880\n",
            "Iteration 303, loss = 0.17734514\n",
            "Iteration 304, loss = 0.17663302\n",
            "Iteration 305, loss = 0.17587923\n",
            "Iteration 306, loss = 0.17513032\n",
            "Iteration 307, loss = 0.17441469\n",
            "Iteration 308, loss = 0.17365388\n",
            "Iteration 309, loss = 0.17293634\n",
            "Iteration 310, loss = 0.17223876\n",
            "Iteration 311, loss = 0.17145970\n",
            "Iteration 312, loss = 0.17072092\n",
            "Iteration 313, loss = 0.17000545\n",
            "Iteration 314, loss = 0.16928436\n",
            "Iteration 315, loss = 0.16859781\n",
            "Iteration 316, loss = 0.16786904\n",
            "Iteration 317, loss = 0.16718776\n",
            "Iteration 318, loss = 0.16649332\n",
            "Iteration 319, loss = 0.16584018\n",
            "Iteration 320, loss = 0.16509639\n",
            "Iteration 321, loss = 0.16440345\n",
            "Iteration 322, loss = 0.16372043\n",
            "Iteration 323, loss = 0.16304909\n",
            "Iteration 324, loss = 0.16236295\n",
            "Iteration 325, loss = 0.16167986\n",
            "Iteration 326, loss = 0.16106267\n",
            "Iteration 327, loss = 0.16036787\n",
            "Iteration 328, loss = 0.15970185\n",
            "Iteration 329, loss = 0.15902766\n",
            "Iteration 330, loss = 0.15835393\n",
            "Iteration 331, loss = 0.15771001\n",
            "Iteration 332, loss = 0.15711580\n",
            "Iteration 333, loss = 0.15647970\n",
            "Iteration 334, loss = 0.15576582\n",
            "Iteration 335, loss = 0.15512263\n",
            "Iteration 336, loss = 0.15449078\n",
            "Iteration 337, loss = 0.15393529\n",
            "Iteration 338, loss = 0.15327349\n",
            "Iteration 339, loss = 0.15270884\n",
            "Iteration 340, loss = 0.15197326\n",
            "Iteration 341, loss = 0.15135850\n",
            "Iteration 342, loss = 0.15082398\n",
            "Iteration 343, loss = 0.15014454\n",
            "Iteration 344, loss = 0.14950951\n",
            "Iteration 345, loss = 0.14892231\n",
            "Iteration 346, loss = 0.14830239\n",
            "Iteration 347, loss = 0.14772585\n",
            "Iteration 348, loss = 0.14709684\n",
            "Iteration 349, loss = 0.14657569\n",
            "Iteration 350, loss = 0.14589735\n",
            "Iteration 351, loss = 0.14533540\n",
            "Iteration 352, loss = 0.14473936\n",
            "Iteration 353, loss = 0.14415627\n",
            "Iteration 354, loss = 0.14357492\n",
            "Iteration 355, loss = 0.14300616\n",
            "Iteration 356, loss = 0.14240917\n",
            "Iteration 357, loss = 0.14184337\n",
            "Iteration 358, loss = 0.14128466\n",
            "Iteration 359, loss = 0.14071563\n",
            "Iteration 360, loss = 0.14017370\n",
            "Iteration 361, loss = 0.13958167\n",
            "Iteration 362, loss = 0.13903648\n",
            "Iteration 363, loss = 0.13847624\n",
            "Iteration 364, loss = 0.13792894\n",
            "Iteration 365, loss = 0.13735763\n",
            "Iteration 366, loss = 0.13684070\n",
            "Iteration 367, loss = 0.13628508\n",
            "Iteration 368, loss = 0.13575883\n",
            "Iteration 369, loss = 0.13522508\n",
            "Iteration 370, loss = 0.13469330\n",
            "Iteration 371, loss = 0.13413338\n",
            "Iteration 372, loss = 0.13361387\n",
            "Iteration 373, loss = 0.13312538\n",
            "Iteration 374, loss = 0.13256456\n",
            "Iteration 375, loss = 0.13202147\n",
            "Iteration 376, loss = 0.13151555\n",
            "Iteration 377, loss = 0.13101626\n",
            "Iteration 378, loss = 0.13050679\n",
            "Iteration 379, loss = 0.12999237\n",
            "Iteration 380, loss = 0.12947300\n",
            "Iteration 381, loss = 0.12895729\n",
            "Iteration 382, loss = 0.12847984\n",
            "Iteration 383, loss = 0.12795293\n",
            "Iteration 384, loss = 0.12746335\n",
            "Iteration 385, loss = 0.12698657\n",
            "Iteration 386, loss = 0.12652461\n",
            "Iteration 387, loss = 0.12601544\n",
            "Iteration 388, loss = 0.12550695\n",
            "Iteration 389, loss = 0.12501021\n",
            "Iteration 390, loss = 0.12455436\n",
            "Iteration 391, loss = 0.12407876\n",
            "Iteration 392, loss = 0.12359578\n",
            "Iteration 393, loss = 0.12313230\n",
            "Iteration 394, loss = 0.12262832\n",
            "Iteration 395, loss = 0.12216317\n",
            "Iteration 396, loss = 0.12172353\n",
            "Iteration 397, loss = 0.12121367\n",
            "Iteration 398, loss = 0.12075497\n",
            "Iteration 399, loss = 0.12032028\n",
            "Iteration 400, loss = 0.11984279\n",
            "Iteration 401, loss = 0.11940391\n",
            "Iteration 402, loss = 0.11895238\n",
            "Iteration 403, loss = 0.11846369\n",
            "Iteration 404, loss = 0.11800846\n",
            "Iteration 405, loss = 0.11756038\n",
            "Iteration 406, loss = 0.11711722\n",
            "Iteration 407, loss = 0.11674610\n",
            "Iteration 408, loss = 0.11622875\n",
            "Iteration 409, loss = 0.11577612\n",
            "Iteration 410, loss = 0.11538598\n",
            "Iteration 411, loss = 0.11495966\n",
            "Iteration 412, loss = 0.11447791\n",
            "Iteration 413, loss = 0.11404196\n",
            "Iteration 414, loss = 0.11360532\n",
            "Iteration 415, loss = 0.11317705\n",
            "Iteration 416, loss = 0.11274171\n",
            "Iteration 417, loss = 0.11232074\n",
            "Iteration 418, loss = 0.11192300\n",
            "Iteration 419, loss = 0.11149990\n",
            "Iteration 420, loss = 0.11106536\n",
            "Iteration 421, loss = 0.11065284\n",
            "Iteration 422, loss = 0.11025357\n",
            "Iteration 423, loss = 0.10983882\n",
            "Iteration 424, loss = 0.10945981\n",
            "Iteration 425, loss = 0.10901777\n",
            "Iteration 426, loss = 0.10863665\n",
            "Iteration 427, loss = 0.10817853\n",
            "Iteration 428, loss = 0.10778372\n",
            "Iteration 429, loss = 0.10740387\n",
            "Iteration 430, loss = 0.10699354\n",
            "Iteration 431, loss = 0.10660617\n",
            "Iteration 432, loss = 0.10620850\n",
            "Iteration 433, loss = 0.10578205\n",
            "Iteration 434, loss = 0.10541763\n",
            "Iteration 435, loss = 0.10499975\n",
            "Iteration 436, loss = 0.10467961\n",
            "Iteration 437, loss = 0.10431250\n",
            "Iteration 438, loss = 0.10384307\n",
            "Iteration 439, loss = 0.10347668\n",
            "Iteration 440, loss = 0.10312568\n",
            "Iteration 441, loss = 0.10274337\n",
            "Iteration 442, loss = 0.10240166\n",
            "Iteration 443, loss = 0.10197588\n",
            "Iteration 444, loss = 0.10157687\n",
            "Iteration 445, loss = 0.10125294\n",
            "Iteration 446, loss = 0.10083762\n",
            "Iteration 447, loss = 0.10052099\n",
            "Iteration 448, loss = 0.10009289\n",
            "Iteration 449, loss = 0.09976060\n",
            "Iteration 450, loss = 0.09936862\n",
            "Iteration 451, loss = 0.09900353\n",
            "Iteration 452, loss = 0.09870249\n",
            "Iteration 453, loss = 0.09827889\n",
            "Iteration 454, loss = 0.09793413\n",
            "Iteration 455, loss = 0.09758849\n",
            "Iteration 456, loss = 0.09721395\n",
            "Iteration 457, loss = 0.09689171\n",
            "Iteration 458, loss = 0.09652984\n",
            "Iteration 459, loss = 0.09618344\n",
            "Iteration 460, loss = 0.09584490\n",
            "Iteration 461, loss = 0.09546452\n",
            "Iteration 462, loss = 0.09514744\n",
            "Iteration 463, loss = 0.09479806\n",
            "Iteration 464, loss = 0.09452717\n",
            "Iteration 465, loss = 0.09409420\n",
            "Iteration 466, loss = 0.09381880\n",
            "Iteration 467, loss = 0.09345168\n",
            "Iteration 468, loss = 0.09308056\n",
            "Iteration 469, loss = 0.09274310\n",
            "Iteration 470, loss = 0.09246802\n",
            "Iteration 471, loss = 0.09210064\n",
            "Iteration 472, loss = 0.09176597\n",
            "Iteration 473, loss = 0.09146477\n",
            "Iteration 474, loss = 0.09118622\n",
            "Iteration 475, loss = 0.09078601\n",
            "Iteration 476, loss = 0.09046935\n",
            "Iteration 477, loss = 0.09018269\n",
            "Iteration 478, loss = 0.08982829\n",
            "Iteration 479, loss = 0.08951452\n",
            "Iteration 480, loss = 0.08921803\n",
            "Iteration 481, loss = 0.08895839\n",
            "Iteration 482, loss = 0.08857608\n",
            "Iteration 483, loss = 0.08827198\n",
            "Iteration 484, loss = 0.08795032\n",
            "Iteration 485, loss = 0.08766673\n",
            "Iteration 486, loss = 0.08733675\n",
            "Iteration 487, loss = 0.08701902\n",
            "Iteration 488, loss = 0.08671531\n",
            "Iteration 489, loss = 0.08641313\n",
            "Iteration 490, loss = 0.08610437\n",
            "Iteration 491, loss = 0.08580800\n",
            "Iteration 492, loss = 0.08552112\n",
            "Iteration 493, loss = 0.08521731\n",
            "Iteration 494, loss = 0.08489748\n",
            "Iteration 495, loss = 0.08464151\n",
            "Iteration 496, loss = 0.08435057\n",
            "Iteration 497, loss = 0.08401447\n",
            "Iteration 498, loss = 0.08372916\n",
            "Iteration 499, loss = 0.08344323\n",
            "Iteration 500, loss = 0.08316882\n",
            "Iteration 501, loss = 0.08287965\n",
            "Iteration 502, loss = 0.08262365\n",
            "Iteration 503, loss = 0.08229653\n",
            "Iteration 504, loss = 0.08208154\n",
            "Iteration 505, loss = 0.08174256\n",
            "Iteration 506, loss = 0.08146858\n",
            "Iteration 507, loss = 0.08118965\n",
            "Iteration 508, loss = 0.08092692\n",
            "Iteration 509, loss = 0.08057906\n",
            "Iteration 510, loss = 0.08034378\n",
            "Iteration 511, loss = 0.08009447\n",
            "Iteration 512, loss = 0.07983731\n",
            "Iteration 513, loss = 0.07952033\n",
            "Iteration 514, loss = 0.07926341\n",
            "Iteration 515, loss = 0.07894517\n",
            "Iteration 516, loss = 0.07866655\n",
            "Iteration 517, loss = 0.07839036\n",
            "Iteration 518, loss = 0.07814806\n",
            "Iteration 519, loss = 0.07787310\n",
            "Iteration 520, loss = 0.07761959\n",
            "Iteration 521, loss = 0.07736922\n",
            "Iteration 522, loss = 0.07712684\n",
            "Iteration 523, loss = 0.07679544\n",
            "Iteration 524, loss = 0.07653993\n",
            "Iteration 525, loss = 0.07629540\n",
            "Iteration 526, loss = 0.07606934\n",
            "Iteration 527, loss = 0.07585177\n",
            "Iteration 528, loss = 0.07552303\n",
            "Iteration 529, loss = 0.07524721\n",
            "Iteration 530, loss = 0.07500884\n",
            "Iteration 531, loss = 0.07474586\n",
            "Iteration 532, loss = 0.07455457\n",
            "Iteration 533, loss = 0.07424569\n",
            "Iteration 534, loss = 0.07403919\n",
            "Iteration 535, loss = 0.07374242\n",
            "Iteration 536, loss = 0.07348032\n",
            "Iteration 537, loss = 0.07325493\n",
            "Iteration 538, loss = 0.07301411\n",
            "Iteration 539, loss = 0.07276898\n",
            "Iteration 540, loss = 0.07250480\n",
            "Iteration 541, loss = 0.07227712\n",
            "Iteration 542, loss = 0.07201784\n",
            "Iteration 543, loss = 0.07179000\n",
            "Iteration 544, loss = 0.07156073\n",
            "Iteration 545, loss = 0.07129254\n",
            "Iteration 546, loss = 0.07110022\n",
            "Iteration 547, loss = 0.07082728\n",
            "Iteration 548, loss = 0.07065244\n",
            "Iteration 549, loss = 0.07036608\n",
            "Iteration 550, loss = 0.07011827\n",
            "Iteration 551, loss = 0.06990452\n",
            "Iteration 552, loss = 0.06966737\n",
            "Iteration 553, loss = 0.06940566\n",
            "Iteration 554, loss = 0.06917419\n",
            "Iteration 555, loss = 0.06896038\n",
            "Iteration 556, loss = 0.06875812\n",
            "Iteration 557, loss = 0.06853631\n",
            "Iteration 558, loss = 0.06826958\n",
            "Iteration 559, loss = 0.06805835\n",
            "Iteration 560, loss = 0.06781637\n",
            "Iteration 561, loss = 0.06760624\n",
            "Iteration 562, loss = 0.06738621\n",
            "Iteration 563, loss = 0.06714180\n",
            "Iteration 564, loss = 0.06691057\n",
            "Iteration 565, loss = 0.06670031\n",
            "Iteration 566, loss = 0.06651222\n",
            "Iteration 567, loss = 0.06625619\n",
            "Iteration 568, loss = 0.06604779\n",
            "Iteration 569, loss = 0.06583719\n",
            "Iteration 570, loss = 0.06561661\n",
            "Iteration 571, loss = 0.06540660\n",
            "Iteration 572, loss = 0.06520071\n",
            "Iteration 573, loss = 0.06498873\n",
            "Iteration 574, loss = 0.06477221\n",
            "Iteration 575, loss = 0.06455285\n",
            "Iteration 576, loss = 0.06438080\n",
            "Iteration 577, loss = 0.06419620\n",
            "Iteration 578, loss = 0.06392085\n",
            "Iteration 579, loss = 0.06371628\n",
            "Iteration 580, loss = 0.06350802\n",
            "Iteration 581, loss = 0.06330368\n",
            "Iteration 582, loss = 0.06307604\n",
            "Iteration 583, loss = 0.06290483\n",
            "Iteration 584, loss = 0.06270419\n",
            "Iteration 585, loss = 0.06247341\n",
            "Iteration 586, loss = 0.06233662\n",
            "Iteration 587, loss = 0.06212756\n",
            "Iteration 588, loss = 0.06188038\n",
            "Iteration 589, loss = 0.06169205\n",
            "Iteration 590, loss = 0.06147557\n",
            "Iteration 591, loss = 0.06129592\n",
            "Iteration 592, loss = 0.06108511\n",
            "Iteration 593, loss = 0.06089162\n",
            "Iteration 594, loss = 0.06071995\n",
            "Iteration 595, loss = 0.06051581\n",
            "Iteration 596, loss = 0.06029061\n",
            "Iteration 597, loss = 0.06010545\n",
            "Iteration 598, loss = 0.06004939\n",
            "Iteration 599, loss = 0.05976249\n",
            "Iteration 600, loss = 0.05954056\n",
            "Iteration 601, loss = 0.05931277\n",
            "Iteration 602, loss = 0.05921384\n",
            "Iteration 603, loss = 0.05896362\n",
            "Iteration 604, loss = 0.05877828\n",
            "Iteration 605, loss = 0.05858544\n",
            "Iteration 606, loss = 0.05849575\n",
            "Iteration 607, loss = 0.05826301\n",
            "Iteration 608, loss = 0.05806327\n",
            "Iteration 609, loss = 0.05784109\n",
            "Iteration 610, loss = 0.05767072\n",
            "Iteration 611, loss = 0.05745592\n",
            "Iteration 612, loss = 0.05732486\n",
            "Iteration 613, loss = 0.05716317\n",
            "Iteration 614, loss = 0.05697007\n",
            "Iteration 615, loss = 0.05673583\n",
            "Iteration 616, loss = 0.05656629\n",
            "Iteration 617, loss = 0.05642582\n",
            "Iteration 618, loss = 0.05624470\n",
            "Iteration 619, loss = 0.05604669\n",
            "Iteration 620, loss = 0.05588280\n",
            "Iteration 621, loss = 0.05567992\n",
            "Iteration 622, loss = 0.05557938\n",
            "Iteration 623, loss = 0.05535296\n",
            "Iteration 624, loss = 0.05517234\n",
            "Iteration 625, loss = 0.05502318\n",
            "Iteration 626, loss = 0.05483024\n",
            "Iteration 627, loss = 0.05465668\n",
            "Iteration 628, loss = 0.05446095\n",
            "Iteration 629, loss = 0.05432934\n",
            "Iteration 630, loss = 0.05414764\n",
            "Iteration 631, loss = 0.05396568\n",
            "Iteration 632, loss = 0.05380988\n",
            "Iteration 633, loss = 0.05362535\n",
            "Iteration 634, loss = 0.05347375\n",
            "Iteration 635, loss = 0.05331467\n",
            "Iteration 636, loss = 0.05315499\n",
            "Iteration 637, loss = 0.05300857\n",
            "Iteration 638, loss = 0.05284709\n",
            "Iteration 639, loss = 0.05264303\n",
            "Iteration 640, loss = 0.05246498\n",
            "Iteration 641, loss = 0.05230919\n",
            "Iteration 642, loss = 0.05219963\n",
            "Iteration 643, loss = 0.05212446\n",
            "Iteration 644, loss = 0.05185485\n",
            "Iteration 645, loss = 0.05168351\n",
            "Iteration 646, loss = 0.05148236\n",
            "Iteration 647, loss = 0.05139648\n",
            "Iteration 648, loss = 0.05119606\n",
            "Iteration 649, loss = 0.05103790\n",
            "Iteration 650, loss = 0.05087896\n",
            "Iteration 651, loss = 0.05072638\n",
            "Iteration 652, loss = 0.05055064\n",
            "Iteration 653, loss = 0.05041985\n",
            "Iteration 654, loss = 0.05028238\n",
            "Iteration 655, loss = 0.05013048\n",
            "Iteration 656, loss = 0.04995351\n",
            "Iteration 657, loss = 0.04978485\n",
            "Iteration 658, loss = 0.04964842\n",
            "Iteration 659, loss = 0.04951502\n",
            "Iteration 660, loss = 0.04934129\n",
            "Iteration 661, loss = 0.04918438\n",
            "Iteration 662, loss = 0.04903120\n",
            "Iteration 663, loss = 0.04893657\n",
            "Iteration 664, loss = 0.04875049\n",
            "Iteration 665, loss = 0.04862310\n",
            "Iteration 666, loss = 0.04850448\n",
            "Iteration 667, loss = 0.04832484\n",
            "Iteration 668, loss = 0.04814181\n",
            "Iteration 669, loss = 0.04799026\n",
            "Iteration 670, loss = 0.04785490\n",
            "Iteration 671, loss = 0.04770148\n",
            "Iteration 672, loss = 0.04759083\n",
            "Iteration 673, loss = 0.04748689\n",
            "Iteration 674, loss = 0.04728828\n",
            "Iteration 675, loss = 0.04713660\n",
            "Iteration 676, loss = 0.04701892\n",
            "Iteration 677, loss = 0.04691248\n",
            "Iteration 678, loss = 0.04674472\n",
            "Iteration 679, loss = 0.04656783\n",
            "Iteration 680, loss = 0.04644934\n",
            "Iteration 681, loss = 0.04629334\n",
            "Iteration 682, loss = 0.04616085\n",
            "Iteration 683, loss = 0.04602912\n",
            "Iteration 684, loss = 0.04586392\n",
            "Iteration 685, loss = 0.04574479\n",
            "Iteration 686, loss = 0.04560433\n",
            "Iteration 687, loss = 0.04546888\n",
            "Iteration 688, loss = 0.04531981\n",
            "Iteration 689, loss = 0.04516976\n",
            "Iteration 690, loss = 0.04509362\n",
            "Iteration 691, loss = 0.04493206\n",
            "Iteration 692, loss = 0.04479974\n",
            "Iteration 693, loss = 0.04468471\n",
            "Iteration 694, loss = 0.04452046\n",
            "Iteration 695, loss = 0.04443555\n",
            "Iteration 696, loss = 0.04423909\n",
            "Iteration 697, loss = 0.04416647\n",
            "Iteration 698, loss = 0.04398428\n",
            "Iteration 699, loss = 0.04385824\n",
            "Iteration 700, loss = 0.04374452\n",
            "Iteration 701, loss = 0.04364394\n",
            "Iteration 702, loss = 0.04348626\n",
            "Iteration 703, loss = 0.04333471\n",
            "Iteration 704, loss = 0.04319872\n",
            "Iteration 705, loss = 0.04309019\n",
            "Iteration 706, loss = 0.04299134\n",
            "Iteration 707, loss = 0.04282681\n",
            "Iteration 708, loss = 0.04269972\n",
            "Iteration 709, loss = 0.04256307\n",
            "Iteration 710, loss = 0.04246172\n",
            "Iteration 711, loss = 0.04235300\n",
            "Iteration 712, loss = 0.04220639\n",
            "Iteration 713, loss = 0.04211834\n",
            "Iteration 714, loss = 0.04195064\n",
            "Iteration 715, loss = 0.04183234\n",
            "Iteration 716, loss = 0.04169987\n",
            "Iteration 717, loss = 0.04158010\n",
            "Iteration 718, loss = 0.04151609\n",
            "Iteration 719, loss = 0.04135285\n",
            "Iteration 720, loss = 0.04119824\n",
            "Iteration 721, loss = 0.04113598\n",
            "Iteration 722, loss = 0.04098188\n",
            "Iteration 723, loss = 0.04088990\n",
            "Iteration 724, loss = 0.04073759\n",
            "Iteration 725, loss = 0.04060882\n",
            "Iteration 726, loss = 0.04049598\n",
            "Iteration 727, loss = 0.04036749\n",
            "Iteration 728, loss = 0.04032857\n",
            "Iteration 729, loss = 0.04014103\n",
            "Iteration 730, loss = 0.04000885\n",
            "Iteration 731, loss = 0.03990459\n",
            "Iteration 732, loss = 0.03978234\n",
            "Iteration 733, loss = 0.03969368\n",
            "Iteration 734, loss = 0.03957957\n",
            "Iteration 735, loss = 0.03944301\n",
            "Iteration 736, loss = 0.03935258\n",
            "Iteration 737, loss = 0.03921786\n",
            "Iteration 738, loss = 0.03912639\n",
            "Iteration 739, loss = 0.03897846\n",
            "Iteration 740, loss = 0.03886443\n",
            "Iteration 741, loss = 0.03877051\n",
            "Iteration 742, loss = 0.03865617\n",
            "Iteration 743, loss = 0.03853137\n",
            "Iteration 744, loss = 0.03841766\n",
            "Iteration 745, loss = 0.03832080\n",
            "Iteration 746, loss = 0.03819865\n",
            "Iteration 747, loss = 0.03811103\n",
            "Iteration 748, loss = 0.03798170\n",
            "Iteration 749, loss = 0.03788870\n",
            "Iteration 750, loss = 0.03780587\n",
            "Iteration 751, loss = 0.03767090\n",
            "Iteration 752, loss = 0.03753383\n",
            "Iteration 753, loss = 0.03745385\n",
            "Iteration 754, loss = 0.03734519\n",
            "Iteration 755, loss = 0.03722725\n",
            "Iteration 756, loss = 0.03711321\n",
            "Iteration 757, loss = 0.03702080\n",
            "Iteration 758, loss = 0.03690044\n",
            "Iteration 759, loss = 0.03681195\n",
            "Iteration 760, loss = 0.03668389\n",
            "Iteration 761, loss = 0.03657831\n",
            "Iteration 762, loss = 0.03648054\n",
            "Iteration 763, loss = 0.03637873\n",
            "Iteration 764, loss = 0.03627253\n",
            "Iteration 765, loss = 0.03617193\n",
            "Iteration 766, loss = 0.03610058\n",
            "Iteration 767, loss = 0.03596466\n",
            "Iteration 768, loss = 0.03589556\n",
            "Iteration 769, loss = 0.03575505\n",
            "Iteration 770, loss = 0.03564632\n",
            "Iteration 771, loss = 0.03559620\n",
            "Iteration 772, loss = 0.03545578\n",
            "Iteration 773, loss = 0.03541116\n",
            "Iteration 774, loss = 0.03526264\n",
            "Iteration 775, loss = 0.03515308\n",
            "Iteration 776, loss = 0.03505755\n",
            "Iteration 777, loss = 0.03494630\n",
            "Iteration 778, loss = 0.03485529\n",
            "Iteration 779, loss = 0.03476303\n",
            "Iteration 780, loss = 0.03465945\n",
            "Iteration 781, loss = 0.03455317\n",
            "Iteration 782, loss = 0.03446297\n",
            "Iteration 783, loss = 0.03436810\n",
            "Iteration 784, loss = 0.03425800\n",
            "Iteration 785, loss = 0.03418243\n",
            "Iteration 786, loss = 0.03409476\n",
            "Iteration 787, loss = 0.03397736\n",
            "Iteration 788, loss = 0.03388913\n",
            "Iteration 789, loss = 0.03379578\n",
            "Iteration 790, loss = 0.03372357\n",
            "Iteration 791, loss = 0.03363295\n",
            "Iteration 792, loss = 0.03350930\n",
            "Iteration 793, loss = 0.03341891\n",
            "Iteration 794, loss = 0.03335157\n",
            "Iteration 795, loss = 0.03323262\n",
            "Iteration 796, loss = 0.03313781\n",
            "Iteration 797, loss = 0.03303704\n",
            "Iteration 798, loss = 0.03296728\n",
            "Iteration 799, loss = 0.03292573\n",
            "Iteration 800, loss = 0.03276934\n",
            "Iteration 801, loss = 0.03266496\n",
            "Iteration 802, loss = 0.03257912\n",
            "Iteration 803, loss = 0.03249401\n",
            "Iteration 804, loss = 0.03241870\n",
            "Iteration 805, loss = 0.03229985\n",
            "Iteration 806, loss = 0.03224233\n",
            "Iteration 807, loss = 0.03213606\n",
            "Iteration 808, loss = 0.03207010\n",
            "Iteration 809, loss = 0.03195297\n",
            "Iteration 810, loss = 0.03190257\n",
            "Iteration 811, loss = 0.03177527\n",
            "Iteration 812, loss = 0.03164984\n",
            "Iteration 813, loss = 0.03158361\n",
            "Iteration 814, loss = 0.03162662\n",
            "Iteration 815, loss = 0.03146758\n",
            "Iteration 816, loss = 0.03148450\n",
            "Iteration 817, loss = 0.03129776\n",
            "Iteration 818, loss = 0.03115432\n",
            "Iteration 819, loss = 0.03110841\n",
            "Iteration 820, loss = 0.03099512\n",
            "Iteration 821, loss = 0.03090405\n",
            "Iteration 822, loss = 0.03084934\n",
            "Iteration 823, loss = 0.03076486\n",
            "Iteration 824, loss = 0.03065329\n",
            "Iteration 825, loss = 0.03056025\n",
            "Iteration 826, loss = 0.03048458\n",
            "Iteration 827, loss = 0.03039130\n",
            "Iteration 828, loss = 0.03033472\n",
            "Iteration 829, loss = 0.03024199\n",
            "Iteration 830, loss = 0.03015219\n",
            "Iteration 831, loss = 0.03003635\n",
            "Iteration 832, loss = 0.02999398\n",
            "Iteration 833, loss = 0.02994505\n",
            "Iteration 834, loss = 0.02982510\n",
            "Iteration 835, loss = 0.02973900\n",
            "Iteration 836, loss = 0.02964984\n",
            "Iteration 837, loss = 0.02958316\n",
            "Iteration 838, loss = 0.02957330\n",
            "Iteration 839, loss = 0.02945407\n",
            "Iteration 840, loss = 0.02937217\n",
            "Iteration 841, loss = 0.02927486\n",
            "Iteration 842, loss = 0.02918355\n",
            "Iteration 843, loss = 0.02911560\n",
            "Iteration 844, loss = 0.02906222\n",
            "Iteration 845, loss = 0.02894402\n",
            "Iteration 846, loss = 0.02890302\n",
            "Iteration 847, loss = 0.02881533\n",
            "Iteration 848, loss = 0.02871933\n",
            "Iteration 849, loss = 0.02867809\n",
            "Iteration 850, loss = 0.02854411\n",
            "Iteration 851, loss = 0.02848461\n",
            "Iteration 852, loss = 0.02841887\n",
            "Iteration 853, loss = 0.02836020\n",
            "Iteration 854, loss = 0.02831319\n",
            "Iteration 855, loss = 0.02820628\n",
            "Iteration 856, loss = 0.02820317\n",
            "Iteration 857, loss = 0.02803353\n",
            "Iteration 858, loss = 0.02794069\n",
            "Iteration 859, loss = 0.02786546\n",
            "Iteration 860, loss = 0.02780179\n",
            "Iteration 861, loss = 0.02778693\n",
            "Iteration 862, loss = 0.02764907\n",
            "Iteration 863, loss = 0.02760855\n",
            "Iteration 864, loss = 0.02749872\n",
            "Iteration 865, loss = 0.02743880\n",
            "Iteration 866, loss = 0.02736963\n",
            "Iteration 867, loss = 0.02727807\n",
            "Iteration 868, loss = 0.02721866\n",
            "Iteration 869, loss = 0.02715319\n",
            "Iteration 870, loss = 0.02706906\n",
            "Iteration 871, loss = 0.02706250\n",
            "Iteration 872, loss = 0.02693170\n",
            "Iteration 873, loss = 0.02686236\n",
            "Iteration 874, loss = 0.02679944\n",
            "Iteration 875, loss = 0.02684835\n",
            "Iteration 876, loss = 0.02660739\n",
            "Iteration 877, loss = 0.02660697\n",
            "Iteration 878, loss = 0.02656888\n",
            "Iteration 879, loss = 0.02643779\n",
            "Iteration 880, loss = 0.02644316\n",
            "Iteration 881, loss = 0.02629523\n",
            "Iteration 882, loss = 0.02633139\n",
            "Iteration 883, loss = 0.02615230\n",
            "Iteration 884, loss = 0.02608545\n",
            "Iteration 885, loss = 0.02600312\n",
            "Iteration 886, loss = 0.02597896\n",
            "Iteration 887, loss = 0.02587473\n",
            "Iteration 888, loss = 0.02580826\n",
            "Iteration 889, loss = 0.02578887\n",
            "Iteration 890, loss = 0.02567682\n",
            "Iteration 891, loss = 0.02560463\n",
            "Iteration 892, loss = 0.02560016\n",
            "Iteration 893, loss = 0.02550179\n",
            "Iteration 894, loss = 0.02539874\n",
            "Iteration 895, loss = 0.02539789\n",
            "Iteration 896, loss = 0.02531822\n",
            "Iteration 897, loss = 0.02523198\n",
            "Iteration 898, loss = 0.02517319\n",
            "Iteration 899, loss = 0.02509826\n",
            "Iteration 900, loss = 0.02502712\n",
            "Iteration 901, loss = 0.02500427\n",
            "Iteration 902, loss = 0.02488275\n",
            "Iteration 903, loss = 0.02482829\n",
            "Iteration 904, loss = 0.02475748\n",
            "Iteration 905, loss = 0.02470134\n",
            "Iteration 906, loss = 0.02467467\n",
            "Iteration 907, loss = 0.02456302\n",
            "Iteration 908, loss = 0.02456955\n",
            "Iteration 909, loss = 0.02444367\n",
            "Iteration 910, loss = 0.02437156\n",
            "Iteration 911, loss = 0.02433040\n",
            "Iteration 912, loss = 0.02424646\n",
            "Iteration 913, loss = 0.02420453\n",
            "Iteration 914, loss = 0.02412456\n",
            "Iteration 915, loss = 0.02404228\n",
            "Iteration 916, loss = 0.02401346\n",
            "Iteration 917, loss = 0.02394521\n",
            "Iteration 918, loss = 0.02391562\n",
            "Iteration 919, loss = 0.02380950\n",
            "Iteration 920, loss = 0.02375646\n",
            "Iteration 921, loss = 0.02371179\n",
            "Iteration 922, loss = 0.02362770\n",
            "Iteration 923, loss = 0.02356123\n",
            "Iteration 924, loss = 0.02356421\n",
            "Iteration 925, loss = 0.02358084\n",
            "Iteration 926, loss = 0.02348485\n",
            "Iteration 927, loss = 0.02339975\n",
            "Iteration 928, loss = 0.02327789\n",
            "Iteration 929, loss = 0.02325596\n",
            "Iteration 930, loss = 0.02317527\n",
            "Iteration 931, loss = 0.02310742\n",
            "Iteration 932, loss = 0.02306696\n",
            "Iteration 933, loss = 0.02298547\n",
            "Iteration 934, loss = 0.02292992\n",
            "Iteration 935, loss = 0.02288123\n",
            "Iteration 936, loss = 0.02280106\n",
            "Iteration 937, loss = 0.02277054\n",
            "Iteration 938, loss = 0.02270021\n",
            "Iteration 939, loss = 0.02268613\n",
            "Iteration 940, loss = 0.02257424\n",
            "Iteration 941, loss = 0.02252613\n",
            "Iteration 942, loss = 0.02251104\n",
            "Iteration 943, loss = 0.02250249\n",
            "Iteration 944, loss = 0.02236105\n",
            "Iteration 945, loss = 0.02231206\n",
            "Iteration 946, loss = 0.02225696\n",
            "Iteration 947, loss = 0.02217867\n",
            "Iteration 948, loss = 0.02211040\n",
            "Iteration 949, loss = 0.02209097\n",
            "Iteration 950, loss = 0.02204062\n",
            "Iteration 951, loss = 0.02196049\n",
            "Iteration 952, loss = 0.02192622\n",
            "Iteration 953, loss = 0.02185295\n",
            "Iteration 954, loss = 0.02179325\n",
            "Iteration 955, loss = 0.02174633\n",
            "Iteration 956, loss = 0.02168881\n",
            "Iteration 957, loss = 0.02167456\n",
            "Iteration 958, loss = 0.02162718\n",
            "Iteration 959, loss = 0.02152488\n",
            "Iteration 960, loss = 0.02145279\n",
            "Iteration 961, loss = 0.02143102\n",
            "Iteration 962, loss = 0.02138770\n",
            "Iteration 963, loss = 0.02133551\n",
            "Iteration 964, loss = 0.02130012\n",
            "Iteration 965, loss = 0.02127753\n",
            "Iteration 966, loss = 0.02133378\n",
            "Iteration 967, loss = 0.02112328\n",
            "Iteration 968, loss = 0.02106639\n",
            "Iteration 969, loss = 0.02099674\n",
            "Iteration 970, loss = 0.02094117\n",
            "Iteration 971, loss = 0.02091034\n",
            "Iteration 972, loss = 0.02083147\n",
            "Iteration 973, loss = 0.02079354\n",
            "Iteration 974, loss = 0.02073657\n",
            "Iteration 975, loss = 0.02070159\n",
            "Iteration 976, loss = 0.02065515\n",
            "Iteration 977, loss = 0.02057579\n",
            "Iteration 978, loss = 0.02054885\n",
            "Iteration 979, loss = 0.02049489\n",
            "Iteration 980, loss = 0.02044682\n",
            "Iteration 981, loss = 0.02037785\n",
            "Iteration 982, loss = 0.02035173\n",
            "Iteration 983, loss = 0.02029050\n",
            "Iteration 984, loss = 0.02026812\n",
            "Iteration 985, loss = 0.02017560\n",
            "Iteration 986, loss = 0.02013220\n",
            "Iteration 987, loss = 0.02008511\n",
            "Iteration 988, loss = 0.02002950\n",
            "Iteration 989, loss = 0.02002467\n",
            "Iteration 990, loss = 0.01994152\n",
            "Iteration 991, loss = 0.01990945\n",
            "Iteration 992, loss = 0.01984436\n",
            "Iteration 993, loss = 0.01980125\n",
            "Iteration 994, loss = 0.01974174\n",
            "Iteration 995, loss = 0.01971958\n",
            "Iteration 996, loss = 0.01968162\n",
            "Iteration 997, loss = 0.01959435\n",
            "Iteration 998, loss = 0.01956834\n",
            "Iteration 999, loss = 0.01949726\n",
            "Iteration 1000, loss = 0.01947447\n",
            "Iteration 1001, loss = 0.01942055\n",
            "Iteration 1002, loss = 0.01934616\n",
            "Iteration 1003, loss = 0.01938286\n",
            "Iteration 1004, loss = 0.01931270\n",
            "Iteration 1005, loss = 0.01922223\n",
            "Iteration 1006, loss = 0.01918080\n",
            "Iteration 1007, loss = 0.01913852\n",
            "Iteration 1008, loss = 0.01909546\n",
            "Iteration 1009, loss = 0.01903654\n",
            "Iteration 1010, loss = 0.01902270\n",
            "Iteration 1011, loss = 0.01893976\n",
            "Iteration 1012, loss = 0.01892242\n",
            "Iteration 1013, loss = 0.01888324\n",
            "Iteration 1014, loss = 0.01883319\n",
            "Iteration 1015, loss = 0.01877325\n",
            "Iteration 1016, loss = 0.01875113\n",
            "Iteration 1017, loss = 0.01870162\n",
            "Iteration 1018, loss = 0.01865107\n",
            "Iteration 1019, loss = 0.01858074\n",
            "Iteration 1020, loss = 0.01856970\n",
            "Iteration 1021, loss = 0.01852068\n",
            "Iteration 1022, loss = 0.01849256\n",
            "Iteration 1023, loss = 0.01847303\n",
            "Iteration 1024, loss = 0.01837780\n",
            "Iteration 1025, loss = 0.01831386\n",
            "Iteration 1026, loss = 0.01829874\n",
            "Iteration 1027, loss = 0.01824719\n",
            "Iteration 1028, loss = 0.01821402\n",
            "Iteration 1029, loss = 0.01823144\n",
            "Iteration 1030, loss = 0.01814551\n",
            "Iteration 1031, loss = 0.01809249\n",
            "Iteration 1032, loss = 0.01802851\n",
            "Iteration 1033, loss = 0.01800126\n",
            "Iteration 1034, loss = 0.01795467\n",
            "Iteration 1035, loss = 0.01791336\n",
            "Iteration 1036, loss = 0.01785770\n",
            "Iteration 1037, loss = 0.01783901\n",
            "Iteration 1038, loss = 0.01778104\n",
            "Iteration 1039, loss = 0.01773764\n",
            "Iteration 1040, loss = 0.01772802\n",
            "Iteration 1041, loss = 0.01770350\n",
            "Iteration 1042, loss = 0.01763823\n",
            "Iteration 1043, loss = 0.01756219\n",
            "Iteration 1044, loss = 0.01753887\n",
            "Iteration 1045, loss = 0.01748377\n",
            "Iteration 1046, loss = 0.01749520\n",
            "Iteration 1047, loss = 0.01741398\n",
            "Iteration 1048, loss = 0.01735271\n",
            "Iteration 1049, loss = 0.01730308\n",
            "Iteration 1050, loss = 0.01727594\n",
            "Iteration 1051, loss = 0.01722359\n",
            "Iteration 1052, loss = 0.01722283\n",
            "Iteration 1053, loss = 0.01714404\n",
            "Iteration 1054, loss = 0.01711589\n",
            "Iteration 1055, loss = 0.01709251\n",
            "Iteration 1056, loss = 0.01703881\n",
            "Iteration 1057, loss = 0.01699797\n",
            "Iteration 1058, loss = 0.01695666\n",
            "Iteration 1059, loss = 0.01691853\n",
            "Iteration 1060, loss = 0.01689428\n",
            "Iteration 1061, loss = 0.01684899\n",
            "Iteration 1062, loss = 0.01686156\n",
            "Iteration 1063, loss = 0.01678745\n",
            "Iteration 1064, loss = 0.01673683\n",
            "Iteration 1065, loss = 0.01669976\n",
            "Iteration 1066, loss = 0.01667799\n",
            "Iteration 1067, loss = 0.01661020\n",
            "Iteration 1068, loss = 0.01662385\n",
            "Iteration 1069, loss = 0.01653641\n",
            "Iteration 1070, loss = 0.01649785\n",
            "Iteration 1071, loss = 0.01646393\n",
            "Iteration 1072, loss = 0.01642748\n",
            "Iteration 1073, loss = 0.01640360\n",
            "Iteration 1074, loss = 0.01637441\n",
            "Iteration 1075, loss = 0.01632466\n",
            "Iteration 1076, loss = 0.01628665\n",
            "Iteration 1077, loss = 0.01623145\n",
            "Iteration 1078, loss = 0.01619271\n",
            "Iteration 1079, loss = 0.01617989\n",
            "Iteration 1080, loss = 0.01611906\n",
            "Iteration 1081, loss = 0.01608088\n",
            "Iteration 1082, loss = 0.01602894\n",
            "Iteration 1083, loss = 0.01605975\n",
            "Iteration 1084, loss = 0.01603308\n",
            "Iteration 1085, loss = 0.01595443\n",
            "Iteration 1086, loss = 0.01589784\n",
            "Iteration 1087, loss = 0.01585575\n",
            "Iteration 1088, loss = 0.01582973\n",
            "Iteration 1089, loss = 0.01577948\n",
            "Iteration 1090, loss = 0.01577505\n",
            "Iteration 1091, loss = 0.01574630\n",
            "Iteration 1092, loss = 0.01571733\n",
            "Iteration 1093, loss = 0.01564173\n",
            "Iteration 1094, loss = 0.01564766\n",
            "Iteration 1095, loss = 0.01558142\n",
            "Iteration 1096, loss = 0.01557999\n",
            "Iteration 1097, loss = 0.01553263\n",
            "Iteration 1098, loss = 0.01552300\n",
            "Iteration 1099, loss = 0.01553372\n",
            "Iteration 1100, loss = 0.01550077\n",
            "Iteration 1101, loss = 0.01539243\n",
            "Iteration 1102, loss = 0.01533285\n",
            "Iteration 1103, loss = 0.01541555\n",
            "Iteration 1104, loss = 0.01526370\n",
            "Iteration 1105, loss = 0.01523936\n",
            "Iteration 1106, loss = 0.01522185\n",
            "Iteration 1107, loss = 0.01517871\n",
            "Iteration 1108, loss = 0.01521092\n",
            "Iteration 1109, loss = 0.01509191\n",
            "Iteration 1110, loss = 0.01508996\n",
            "Iteration 1111, loss = 0.01505044\n",
            "Iteration 1112, loss = 0.01502055\n",
            "Iteration 1113, loss = 0.01498613\n",
            "Iteration 1114, loss = 0.01493754\n",
            "Iteration 1115, loss = 0.01490043\n",
            "Iteration 1116, loss = 0.01487779\n",
            "Iteration 1117, loss = 0.01486633\n",
            "Iteration 1118, loss = 0.01481534\n",
            "Iteration 1119, loss = 0.01476557\n",
            "Iteration 1120, loss = 0.01473603\n",
            "Iteration 1121, loss = 0.01473465\n",
            "Iteration 1122, loss = 0.01469291\n",
            "Iteration 1123, loss = 0.01465888\n",
            "Iteration 1124, loss = 0.01462916\n",
            "Iteration 1125, loss = 0.01459293\n",
            "Iteration 1126, loss = 0.01461019\n",
            "Iteration 1127, loss = 0.01452221\n",
            "Iteration 1128, loss = 0.01448072\n",
            "Iteration 1129, loss = 0.01445188\n",
            "Iteration 1130, loss = 0.01442234\n",
            "Iteration 1131, loss = 0.01437391\n",
            "Iteration 1132, loss = 0.01437646\n",
            "Iteration 1133, loss = 0.01432436\n",
            "Iteration 1134, loss = 0.01430710\n",
            "Iteration 1135, loss = 0.01426376\n",
            "Iteration 1136, loss = 0.01422276\n",
            "Iteration 1137, loss = 0.01420350\n",
            "Iteration 1138, loss = 0.01419971\n",
            "Iteration 1139, loss = 0.01411971\n",
            "Iteration 1140, loss = 0.01413082\n",
            "Iteration 1141, loss = 0.01407261\n",
            "Iteration 1142, loss = 0.01404238\n",
            "Iteration 1143, loss = 0.01408320\n",
            "Iteration 1144, loss = 0.01400626\n",
            "Iteration 1145, loss = 0.01396356\n",
            "Iteration 1146, loss = 0.01393031\n",
            "Iteration 1147, loss = 0.01390064\n",
            "Iteration 1148, loss = 0.01388175\n",
            "Iteration 1149, loss = 0.01384357\n",
            "Iteration 1150, loss = 0.01379587\n",
            "Iteration 1151, loss = 0.01378354\n",
            "Iteration 1152, loss = 0.01374478\n",
            "Iteration 1153, loss = 0.01372458\n",
            "Iteration 1154, loss = 0.01371118\n",
            "Iteration 1155, loss = 0.01373111\n",
            "Iteration 1156, loss = 0.01363877\n",
            "Iteration 1157, loss = 0.01359931\n",
            "Iteration 1158, loss = 0.01358723\n",
            "Iteration 1159, loss = 0.01365007\n",
            "Iteration 1160, loss = 0.01350936\n",
            "Iteration 1161, loss = 0.01348260\n",
            "Iteration 1162, loss = 0.01345837\n",
            "Iteration 1163, loss = 0.01342811\n",
            "Iteration 1164, loss = 0.01339995\n",
            "Iteration 1165, loss = 0.01337408\n",
            "Iteration 1166, loss = 0.01335160\n",
            "Iteration 1167, loss = 0.01336042\n",
            "Iteration 1168, loss = 0.01329074\n",
            "Iteration 1169, loss = 0.01325519\n",
            "Iteration 1170, loss = 0.01323541\n",
            "Iteration 1171, loss = 0.01321189\n",
            "Iteration 1172, loss = 0.01316446\n",
            "Iteration 1173, loss = 0.01316224\n",
            "Iteration 1174, loss = 0.01314204\n",
            "Iteration 1175, loss = 0.01309568\n",
            "Iteration 1176, loss = 0.01307177\n",
            "Iteration 1177, loss = 0.01303482\n",
            "Iteration 1178, loss = 0.01301935\n",
            "Iteration 1179, loss = 0.01301068\n",
            "Iteration 1180, loss = 0.01299263\n",
            "Iteration 1181, loss = 0.01295870\n",
            "Iteration 1182, loss = 0.01290073\n",
            "Iteration 1183, loss = 0.01286333\n",
            "Iteration 1184, loss = 0.01285871\n",
            "Iteration 1185, loss = 0.01281207\n",
            "Iteration 1186, loss = 0.01278658\n",
            "Iteration 1187, loss = 0.01277242\n",
            "Iteration 1188, loss = 0.01274553\n",
            "Iteration 1189, loss = 0.01269825\n",
            "Iteration 1190, loss = 0.01269147\n",
            "Iteration 1191, loss = 0.01266988\n",
            "Iteration 1192, loss = 0.01263046\n",
            "Iteration 1193, loss = 0.01261439\n",
            "Iteration 1194, loss = 0.01257750\n",
            "Iteration 1195, loss = 0.01254767\n",
            "Iteration 1196, loss = 0.01255449\n",
            "Iteration 1197, loss = 0.01252656\n",
            "Iteration 1198, loss = 0.01248538\n",
            "Iteration 1199, loss = 0.01249023\n",
            "Iteration 1200, loss = 0.01242374\n",
            "Iteration 1201, loss = 0.01240124\n",
            "Iteration 1202, loss = 0.01237333\n",
            "Iteration 1203, loss = 0.01234332\n",
            "Iteration 1204, loss = 0.01232666\n",
            "Iteration 1205, loss = 0.01229936\n",
            "Iteration 1206, loss = 0.01229920\n",
            "Iteration 1207, loss = 0.01226185\n",
            "Iteration 1208, loss = 0.01222576\n",
            "Iteration 1209, loss = 0.01219619\n",
            "Iteration 1210, loss = 0.01221941\n",
            "Iteration 1211, loss = 0.01216835\n",
            "Iteration 1212, loss = 0.01211751\n",
            "Iteration 1213, loss = 0.01211802\n",
            "Iteration 1214, loss = 0.01207750\n",
            "Iteration 1215, loss = 0.01206614\n",
            "Iteration 1216, loss = 0.01204381\n",
            "Iteration 1217, loss = 0.01201527\n",
            "Iteration 1218, loss = 0.01198851\n",
            "Iteration 1219, loss = 0.01194398\n",
            "Iteration 1220, loss = 0.01192741\n",
            "Iteration 1221, loss = 0.01190000\n",
            "Iteration 1222, loss = 0.01187514\n",
            "Iteration 1223, loss = 0.01185895\n",
            "Iteration 1224, loss = 0.01183476\n",
            "Iteration 1225, loss = 0.01182324\n",
            "Iteration 1226, loss = 0.01177883\n",
            "Iteration 1227, loss = 0.01175210\n",
            "Iteration 1228, loss = 0.01175992\n",
            "Iteration 1229, loss = 0.01174022\n",
            "Iteration 1230, loss = 0.01173624\n",
            "Iteration 1231, loss = 0.01169392\n",
            "Iteration 1232, loss = 0.01164376\n",
            "Iteration 1233, loss = 0.01161535\n",
            "Iteration 1234, loss = 0.01158313\n",
            "Iteration 1235, loss = 0.01160552\n",
            "Iteration 1236, loss = 0.01153674\n",
            "Iteration 1237, loss = 0.01152458\n",
            "Iteration 1238, loss = 0.01151942\n",
            "Iteration 1239, loss = 0.01152583\n",
            "Iteration 1240, loss = 0.01145663\n",
            "Iteration 1241, loss = 0.01145456\n",
            "Iteration 1242, loss = 0.01144324\n",
            "Iteration 1243, loss = 0.01138726\n",
            "Iteration 1244, loss = 0.01138182\n",
            "Iteration 1245, loss = 0.01135580\n",
            "Iteration 1246, loss = 0.01131313\n",
            "Iteration 1247, loss = 0.01131073\n",
            "Iteration 1248, loss = 0.01129190\n",
            "Iteration 1249, loss = 0.01127511\n",
            "Iteration 1250, loss = 0.01123134\n",
            "Iteration 1251, loss = 0.01122020\n",
            "Iteration 1252, loss = 0.01122288\n",
            "Iteration 1253, loss = 0.01118615\n",
            "Iteration 1254, loss = 0.01114088\n",
            "Iteration 1255, loss = 0.01112223\n",
            "Iteration 1256, loss = 0.01110345\n",
            "Iteration 1257, loss = 0.01110383\n",
            "Iteration 1258, loss = 0.01104890\n",
            "Iteration 1259, loss = 0.01104469\n",
            "Iteration 1260, loss = 0.01103486\n",
            "Iteration 1261, loss = 0.01099580\n",
            "Iteration 1262, loss = 0.01100268\n",
            "Iteration 1263, loss = 0.01096123\n",
            "Iteration 1264, loss = 0.01094531\n",
            "Iteration 1265, loss = 0.01093585\n",
            "Iteration 1266, loss = 0.01088261\n",
            "Iteration 1267, loss = 0.01090020\n",
            "Iteration 1268, loss = 0.01084295\n",
            "Iteration 1269, loss = 0.01084990\n",
            "Iteration 1270, loss = 0.01083277\n",
            "Iteration 1271, loss = 0.01077682\n",
            "Iteration 1272, loss = 0.01076271\n",
            "Iteration 1273, loss = 0.01078401\n",
            "Iteration 1274, loss = 0.01071468\n",
            "Iteration 1275, loss = 0.01071081\n",
            "Iteration 1276, loss = 0.01066905\n",
            "Iteration 1277, loss = 0.01065573\n",
            "Iteration 1278, loss = 0.01064654\n",
            "Iteration 1279, loss = 0.01061338\n",
            "Iteration 1280, loss = 0.01059717\n",
            "Iteration 1281, loss = 0.01063130\n",
            "Iteration 1282, loss = 0.01061042\n",
            "Iteration 1283, loss = 0.01054490\n",
            "Iteration 1284, loss = 0.01052289\n",
            "Iteration 1285, loss = 0.01048975\n",
            "Iteration 1286, loss = 0.01053313\n",
            "Iteration 1287, loss = 0.01044894\n",
            "Iteration 1288, loss = 0.01045875\n",
            "Iteration 1289, loss = 0.01042236\n",
            "Iteration 1290, loss = 0.01043759\n",
            "Iteration 1291, loss = 0.01040200\n",
            "Iteration 1292, loss = 0.01034998\n",
            "Iteration 1293, loss = 0.01034660\n",
            "Iteration 1294, loss = 0.01035550\n",
            "Iteration 1295, loss = 0.01030411\n",
            "Iteration 1296, loss = 0.01027695\n",
            "Iteration 1297, loss = 0.01026964\n",
            "Iteration 1298, loss = 0.01025847\n",
            "Iteration 1299, loss = 0.01020513\n",
            "Iteration 1300, loss = 0.01020892\n",
            "Iteration 1301, loss = 0.01019645\n",
            "Iteration 1302, loss = 0.01015112\n",
            "Iteration 1303, loss = 0.01016287\n",
            "Iteration 1304, loss = 0.01014402\n",
            "Iteration 1305, loss = 0.01012874\n",
            "Iteration 1306, loss = 0.01011222\n",
            "Iteration 1307, loss = 0.01008485\n",
            "Iteration 1308, loss = 0.01006094\n",
            "Iteration 1309, loss = 0.01004673\n",
            "Iteration 1310, loss = 0.01001566\n",
            "Iteration 1311, loss = 0.01007185\n",
            "Iteration 1312, loss = 0.00996576\n",
            "Iteration 1313, loss = 0.00996381\n",
            "Iteration 1314, loss = 0.00995514\n",
            "Iteration 1315, loss = 0.00992751\n",
            "Iteration 1316, loss = 0.00989639\n",
            "Iteration 1317, loss = 0.00990039\n",
            "Iteration 1318, loss = 0.00988948\n",
            "Iteration 1319, loss = 0.00987580\n",
            "Iteration 1320, loss = 0.00988509\n",
            "Iteration 1321, loss = 0.00992485\n",
            "Iteration 1322, loss = 0.00986002\n",
            "Iteration 1323, loss = 0.00978617\n",
            "Iteration 1324, loss = 0.00979698\n",
            "Iteration 1325, loss = 0.00977216\n",
            "Iteration 1326, loss = 0.00971456\n",
            "Iteration 1327, loss = 0.00970608\n",
            "Iteration 1328, loss = 0.00972218\n",
            "Iteration 1329, loss = 0.00969985\n",
            "Iteration 1330, loss = 0.00963226\n",
            "Iteration 1331, loss = 0.00963714\n",
            "Iteration 1332, loss = 0.00966764\n",
            "Iteration 1333, loss = 0.00961091\n",
            "Iteration 1334, loss = 0.00960944\n",
            "Iteration 1335, loss = 0.00961241\n",
            "Iteration 1336, loss = 0.00958880\n",
            "Iteration 1337, loss = 0.00954087\n",
            "Iteration 1338, loss = 0.00950394\n",
            "Iteration 1339, loss = 0.00950798\n",
            "Iteration 1340, loss = 0.00952190\n",
            "Iteration 1341, loss = 0.00944097\n",
            "Iteration 1342, loss = 0.00944674\n",
            "Iteration 1343, loss = 0.00943879\n",
            "Iteration 1344, loss = 0.00941928\n",
            "Iteration 1345, loss = 0.00944984\n",
            "Iteration 1346, loss = 0.00939550\n",
            "Iteration 1347, loss = 0.00939055\n",
            "Iteration 1348, loss = 0.00936219\n",
            "Iteration 1349, loss = 0.00933738\n",
            "Iteration 1350, loss = 0.00935657\n",
            "Iteration 1351, loss = 0.00933430\n",
            "Iteration 1352, loss = 0.00929158\n",
            "Iteration 1353, loss = 0.00934923\n",
            "Iteration 1354, loss = 0.00932031\n",
            "Iteration 1355, loss = 0.00925226\n",
            "Iteration 1356, loss = 0.00921442\n",
            "Iteration 1357, loss = 0.00922752\n",
            "Iteration 1358, loss = 0.00919159\n",
            "Iteration 1359, loss = 0.00916663\n",
            "Iteration 1360, loss = 0.00914803\n",
            "Iteration 1361, loss = 0.00913236\n",
            "Iteration 1362, loss = 0.00913685\n",
            "Iteration 1363, loss = 0.00912692\n",
            "Iteration 1364, loss = 0.00910950\n",
            "Iteration 1365, loss = 0.00909558\n",
            "Iteration 1366, loss = 0.00918600\n",
            "Iteration 1367, loss = 0.00906442\n",
            "Iteration 1368, loss = 0.00902041\n",
            "Iteration 1369, loss = 0.00902506\n",
            "Iteration 1370, loss = 0.00902236\n",
            "Iteration 1371, loss = 0.00898342\n",
            "Iteration 1372, loss = 0.00895983\n",
            "Iteration 1373, loss = 0.00895701\n",
            "Iteration 1374, loss = 0.00892634\n",
            "Iteration 1375, loss = 0.00891016\n",
            "Iteration 1376, loss = 0.00891040\n",
            "Iteration 1377, loss = 0.00890875\n",
            "Iteration 1378, loss = 0.00890797\n",
            "Iteration 1379, loss = 0.00890979\n",
            "Iteration 1380, loss = 0.00883921\n",
            "Iteration 1381, loss = 0.00882758\n",
            "Iteration 1382, loss = 0.00882189\n",
            "Iteration 1383, loss = 0.00881109\n",
            "Iteration 1384, loss = 0.00877036\n",
            "Iteration 1385, loss = 0.00880795\n",
            "Iteration 1386, loss = 0.00875272\n",
            "Iteration 1387, loss = 0.00874736\n",
            "Iteration 1388, loss = 0.00873625\n",
            "Iteration 1389, loss = 0.00873836\n",
            "Iteration 1390, loss = 0.00872370\n",
            "Iteration 1391, loss = 0.00870537\n",
            "Iteration 1392, loss = 0.00866942\n",
            "Iteration 1393, loss = 0.00864410\n",
            "Iteration 1394, loss = 0.00864907\n",
            "Iteration 1395, loss = 0.00872010\n",
            "Iteration 1396, loss = 0.00866131\n",
            "Iteration 1397, loss = 0.00861744\n",
            "Iteration 1398, loss = 0.00861509\n",
            "Iteration 1399, loss = 0.00859728\n",
            "Iteration 1400, loss = 0.00859806\n",
            "Iteration 1401, loss = 0.00854546\n",
            "Iteration 1402, loss = 0.00852025\n",
            "Iteration 1403, loss = 0.00852696\n",
            "Iteration 1404, loss = 0.00849443\n",
            "Iteration 1405, loss = 0.00849837\n",
            "Iteration 1406, loss = 0.00851094\n",
            "Iteration 1407, loss = 0.00851119\n",
            "Iteration 1408, loss = 0.00850388\n",
            "Iteration 1409, loss = 0.00846129\n",
            "Iteration 1410, loss = 0.00841750\n",
            "Iteration 1411, loss = 0.00842709\n",
            "Iteration 1412, loss = 0.00840604\n",
            "Iteration 1413, loss = 0.00839432\n",
            "Iteration 1414, loss = 0.00835310\n",
            "Iteration 1415, loss = 0.00835689\n",
            "Iteration 1416, loss = 0.00835056\n",
            "Iteration 1417, loss = 0.00831544\n",
            "Iteration 1418, loss = 0.00833662\n",
            "Iteration 1419, loss = 0.00833486\n",
            "Iteration 1420, loss = 0.00828482\n",
            "Iteration 1421, loss = 0.00829995\n",
            "Iteration 1422, loss = 0.00824069\n",
            "Iteration 1423, loss = 0.00824972\n",
            "Iteration 1424, loss = 0.00823931\n",
            "Iteration 1425, loss = 0.00820303\n",
            "Iteration 1426, loss = 0.00821723\n",
            "Iteration 1427, loss = 0.00818540\n",
            "Iteration 1428, loss = 0.00820934\n",
            "Iteration 1429, loss = 0.00815064\n",
            "Iteration 1430, loss = 0.00818682\n",
            "Iteration 1431, loss = 0.00817923\n",
            "Iteration 1432, loss = 0.00816092\n",
            "Iteration 1433, loss = 0.00811284\n",
            "Iteration 1434, loss = 0.00812827\n",
            "Iteration 1435, loss = 0.00812958\n",
            "Iteration 1436, loss = 0.00806148\n",
            "Iteration 1437, loss = 0.00806077\n",
            "Iteration 1438, loss = 0.00813865\n",
            "Iteration 1439, loss = 0.00804116\n",
            "Iteration 1440, loss = 0.00804339\n",
            "Iteration 1441, loss = 0.00800512\n",
            "Iteration 1442, loss = 0.00799981\n",
            "Iteration 1443, loss = 0.00801038\n",
            "Iteration 1444, loss = 0.00797679\n",
            "Iteration 1445, loss = 0.00799673\n",
            "Iteration 1446, loss = 0.00795336\n",
            "Iteration 1447, loss = 0.00792921\n",
            "Iteration 1448, loss = 0.00791208\n",
            "Iteration 1449, loss = 0.00791000\n",
            "Iteration 1450, loss = 0.00790728\n",
            "Iteration 1451, loss = 0.00789355\n",
            "Iteration 1452, loss = 0.00786069\n",
            "Iteration 1453, loss = 0.00786032\n",
            "Iteration 1454, loss = 0.00784372\n",
            "Iteration 1455, loss = 0.00783308\n",
            "Iteration 1456, loss = 0.00786984\n",
            "Iteration 1457, loss = 0.00784296\n",
            "Iteration 1458, loss = 0.00783206\n",
            "Iteration 1459, loss = 0.00779868\n",
            "Iteration 1460, loss = 0.00782339\n",
            "Iteration 1461, loss = 0.00776301\n",
            "Iteration 1462, loss = 0.00773834\n",
            "Iteration 1463, loss = 0.00772899\n",
            "Iteration 1464, loss = 0.00772675\n",
            "Iteration 1465, loss = 0.00772277\n",
            "Iteration 1466, loss = 0.00769832\n",
            "Iteration 1467, loss = 0.00772314\n",
            "Iteration 1468, loss = 0.00768913\n",
            "Iteration 1469, loss = 0.00766019\n",
            "Iteration 1470, loss = 0.00765686\n",
            "Iteration 1471, loss = 0.00766498\n",
            "Iteration 1472, loss = 0.00767159\n",
            "Iteration 1473, loss = 0.00767132\n",
            "Iteration 1474, loss = 0.00766006\n",
            "Iteration 1475, loss = 0.00759854\n",
            "Iteration 1476, loss = 0.00759125\n",
            "Iteration 1477, loss = 0.00756447\n",
            "Iteration 1478, loss = 0.00756478\n",
            "Iteration 1479, loss = 0.00757105\n",
            "Iteration 1480, loss = 0.00756135\n",
            "Iteration 1481, loss = 0.00753686\n",
            "Iteration 1482, loss = 0.00751087\n",
            "Iteration 1483, loss = 0.00750409\n",
            "Iteration 1484, loss = 0.00753560\n",
            "Iteration 1485, loss = 0.00750261\n",
            "Iteration 1486, loss = 0.00748983\n",
            "Iteration 1487, loss = 0.00747658\n",
            "Iteration 1488, loss = 0.00748468\n",
            "Iteration 1489, loss = 0.00744555\n",
            "Iteration 1490, loss = 0.00742806\n",
            "Iteration 1491, loss = 0.00742890\n",
            "Iteration 1492, loss = 0.00741106\n",
            "Iteration 1493, loss = 0.00739929\n",
            "Iteration 1494, loss = 0.00744842\n",
            "Iteration 1495, loss = 0.00737952\n",
            "Iteration 1496, loss = 0.00737866\n",
            "Iteration 1497, loss = 0.00736717\n",
            "Iteration 1498, loss = 0.00734396\n",
            "Iteration 1499, loss = 0.00732124\n",
            "Iteration 1500, loss = 0.00733866\n",
            "Iteration 1501, loss = 0.00731271\n",
            "Iteration 1502, loss = 0.00730069\n",
            "Iteration 1503, loss = 0.00729389\n",
            "Iteration 1504, loss = 0.00729662\n",
            "Iteration 1505, loss = 0.00727129\n",
            "Iteration 1506, loss = 0.00727078\n",
            "Iteration 1507, loss = 0.00722621\n",
            "Iteration 1508, loss = 0.00729510\n",
            "Iteration 1509, loss = 0.00724536\n",
            "Iteration 1510, loss = 0.00722767\n",
            "Iteration 1511, loss = 0.00724721\n",
            "Iteration 1512, loss = 0.00720617\n",
            "Iteration 1513, loss = 0.00717544\n",
            "Iteration 1514, loss = 0.00716228\n",
            "Iteration 1515, loss = 0.00716109\n",
            "Iteration 1516, loss = 0.00714503\n",
            "Iteration 1517, loss = 0.00713345\n",
            "Iteration 1518, loss = 0.00712534\n",
            "Iteration 1519, loss = 0.00718213\n",
            "Iteration 1520, loss = 0.00709840\n",
            "Iteration 1521, loss = 0.00712375\n",
            "Iteration 1522, loss = 0.00709114\n",
            "Iteration 1523, loss = 0.00707661\n",
            "Iteration 1524, loss = 0.00706043\n",
            "Iteration 1525, loss = 0.00706702\n",
            "Iteration 1526, loss = 0.00706842\n",
            "Iteration 1527, loss = 0.00706533\n",
            "Iteration 1528, loss = 0.00702239\n",
            "Iteration 1529, loss = 0.00706346\n",
            "Iteration 1530, loss = 0.00704192\n",
            "Iteration 1531, loss = 0.00698273\n",
            "Iteration 1532, loss = 0.00698926\n",
            "Iteration 1533, loss = 0.00700575\n",
            "Iteration 1534, loss = 0.00694570\n",
            "Iteration 1535, loss = 0.00698763\n",
            "Iteration 1536, loss = 0.00698066\n",
            "Iteration 1537, loss = 0.00697237\n",
            "Iteration 1538, loss = 0.00693450\n",
            "Iteration 1539, loss = 0.00694285\n",
            "Iteration 1540, loss = 0.00688766\n",
            "Iteration 1541, loss = 0.00690075\n",
            "Iteration 1542, loss = 0.00689535\n",
            "Iteration 1543, loss = 0.00690177\n",
            "Iteration 1544, loss = 0.00686725\n",
            "Iteration 1545, loss = 0.00685668\n",
            "Iteration 1546, loss = 0.00685678\n",
            "Iteration 1547, loss = 0.00684064\n",
            "Iteration 1548, loss = 0.00682885\n",
            "Iteration 1549, loss = 0.00685856\n",
            "Iteration 1550, loss = 0.00682206\n",
            "Iteration 1551, loss = 0.00679159\n",
            "Iteration 1552, loss = 0.00682993\n",
            "Iteration 1553, loss = 0.00686524\n",
            "Iteration 1554, loss = 0.00678935\n",
            "Iteration 1555, loss = 0.00677022\n",
            "Iteration 1556, loss = 0.00676532\n",
            "Iteration 1557, loss = 0.00674545\n",
            "Iteration 1558, loss = 0.00673750\n",
            "Iteration 1559, loss = 0.00674389\n",
            "Iteration 1560, loss = 0.00672265\n",
            "Iteration 1561, loss = 0.00673152\n",
            "Iteration 1562, loss = 0.00672310\n",
            "Iteration 1563, loss = 0.00668389\n",
            "Iteration 1564, loss = 0.00677919\n",
            "Iteration 1565, loss = 0.00666517\n",
            "Iteration 1566, loss = 0.00666644\n",
            "Iteration 1567, loss = 0.00667036\n",
            "Iteration 1568, loss = 0.00667255\n",
            "Iteration 1569, loss = 0.00663600\n",
            "Iteration 1570, loss = 0.00663866\n",
            "Iteration 1571, loss = 0.00666330\n",
            "Iteration 1572, loss = 0.00665658\n",
            "Iteration 1573, loss = 0.00668107\n",
            "Iteration 1574, loss = 0.00659928\n",
            "Iteration 1575, loss = 0.00659542\n",
            "Iteration 1576, loss = 0.00655377\n",
            "Iteration 1577, loss = 0.00655996\n",
            "Iteration 1578, loss = 0.00661042\n",
            "Iteration 1579, loss = 0.00654528\n",
            "Iteration 1580, loss = 0.00657392\n",
            "Iteration 1581, loss = 0.00654579\n",
            "Iteration 1582, loss = 0.00653901\n",
            "Iteration 1583, loss = 0.00652709\n",
            "Iteration 1584, loss = 0.00651020\n",
            "Iteration 1585, loss = 0.00650797\n",
            "Iteration 1586, loss = 0.00649962\n",
            "Iteration 1587, loss = 0.00648279\n",
            "Iteration 1588, loss = 0.00653746\n",
            "Iteration 1589, loss = 0.00647776\n",
            "Iteration 1590, loss = 0.00646538\n",
            "Iteration 1591, loss = 0.00644502\n",
            "Iteration 1592, loss = 0.00643164\n",
            "Iteration 1593, loss = 0.00643182\n",
            "Iteration 1594, loss = 0.00644627\n",
            "Iteration 1595, loss = 0.00644778\n",
            "Iteration 1596, loss = 0.00641840\n",
            "Iteration 1597, loss = 0.00639916\n",
            "Iteration 1598, loss = 0.00644772\n",
            "Iteration 1599, loss = 0.00638896\n",
            "Iteration 1600, loss = 0.00636232\n",
            "Iteration 1601, loss = 0.00636921\n",
            "Iteration 1602, loss = 0.00637829\n",
            "Iteration 1603, loss = 0.00642227\n",
            "Iteration 1604, loss = 0.00633772\n",
            "Iteration 1605, loss = 0.00631742\n",
            "Iteration 1606, loss = 0.00631518\n",
            "Iteration 1607, loss = 0.00633691\n",
            "Iteration 1608, loss = 0.00631435\n",
            "Iteration 1609, loss = 0.00633882\n",
            "Iteration 1610, loss = 0.00631050\n",
            "Iteration 1611, loss = 0.00628776\n",
            "Iteration 1612, loss = 0.00627005\n",
            "Iteration 1613, loss = 0.00628331\n",
            "Iteration 1614, loss = 0.00630625\n",
            "Iteration 1615, loss = 0.00622679\n",
            "Iteration 1616, loss = 0.00626497\n",
            "Iteration 1617, loss = 0.00628308\n",
            "Iteration 1618, loss = 0.00625845\n",
            "Iteration 1619, loss = 0.00620893\n",
            "Iteration 1620, loss = 0.00622175\n",
            "Iteration 1621, loss = 0.00619417\n",
            "Iteration 1622, loss = 0.00617213\n",
            "Iteration 1623, loss = 0.00617423\n",
            "Iteration 1624, loss = 0.00622544\n",
            "Iteration 1625, loss = 0.00619833\n",
            "Iteration 1626, loss = 0.00616503\n",
            "Iteration 1627, loss = 0.00614639\n",
            "Iteration 1628, loss = 0.00617108\n",
            "Iteration 1629, loss = 0.00615522\n",
            "Iteration 1630, loss = 0.00618330\n",
            "Iteration 1631, loss = 0.00613618\n",
            "Iteration 1632, loss = 0.00611842\n",
            "Iteration 1633, loss = 0.00612782\n",
            "Iteration 1634, loss = 0.00612669\n",
            "Iteration 1635, loss = 0.00610122\n",
            "Iteration 1636, loss = 0.00608501\n",
            "Iteration 1637, loss = 0.00607524\n",
            "Iteration 1638, loss = 0.00606939\n",
            "Iteration 1639, loss = 0.00610369\n",
            "Iteration 1640, loss = 0.00609084\n",
            "Iteration 1641, loss = 0.00607706\n",
            "Iteration 1642, loss = 0.00608286\n",
            "Iteration 1643, loss = 0.00606217\n",
            "Iteration 1644, loss = 0.00602863\n",
            "Iteration 1645, loss = 0.00605241\n",
            "Iteration 1646, loss = 0.00601321\n",
            "Iteration 1647, loss = 0.00601250\n",
            "Iteration 1648, loss = 0.00599309\n",
            "Iteration 1649, loss = 0.00599985\n",
            "Iteration 1650, loss = 0.00599948\n",
            "Iteration 1651, loss = 0.00601186\n",
            "Iteration 1652, loss = 0.00602270\n",
            "Iteration 1653, loss = 0.00601590\n",
            "Iteration 1654, loss = 0.00597297\n",
            "Iteration 1655, loss = 0.00594462\n",
            "Iteration 1656, loss = 0.00593844\n",
            "Iteration 1657, loss = 0.00593806\n",
            "Iteration 1658, loss = 0.00592053\n",
            "Iteration 1659, loss = 0.00592947\n",
            "Iteration 1660, loss = 0.00591051\n",
            "Iteration 1661, loss = 0.00590503\n",
            "Iteration 1662, loss = 0.00590802\n",
            "Iteration 1663, loss = 0.00591464\n",
            "Iteration 1664, loss = 0.00590990\n",
            "Iteration 1665, loss = 0.00591574\n",
            "Iteration 1666, loss = 0.00596834\n",
            "Iteration 1667, loss = 0.00589757\n",
            "Iteration 1668, loss = 0.00585496\n",
            "Iteration 1669, loss = 0.00585940\n",
            "Iteration 1670, loss = 0.00584927\n",
            "Iteration 1671, loss = 0.00582817\n",
            "Iteration 1672, loss = 0.00582047\n",
            "Iteration 1673, loss = 0.00584302\n",
            "Iteration 1674, loss = 0.00582562\n",
            "Iteration 1675, loss = 0.00582697\n",
            "Iteration 1676, loss = 0.00579636\n",
            "Iteration 1677, loss = 0.00577446\n",
            "Iteration 1678, loss = 0.00580567\n",
            "Iteration 1679, loss = 0.00581376\n",
            "Iteration 1680, loss = 0.00577488\n",
            "Iteration 1681, loss = 0.00578010\n",
            "Iteration 1682, loss = 0.00577103\n",
            "Iteration 1683, loss = 0.00576169\n",
            "Iteration 1684, loss = 0.00578951\n",
            "Iteration 1685, loss = 0.00574956\n",
            "Iteration 1686, loss = 0.00571684\n",
            "Iteration 1687, loss = 0.00571148\n",
            "Iteration 1688, loss = 0.00575363\n",
            "Iteration 1689, loss = 0.00576172\n",
            "Iteration 1690, loss = 0.00569520\n",
            "Iteration 1691, loss = 0.00571612\n",
            "Iteration 1692, loss = 0.00573394\n",
            "Iteration 1693, loss = 0.00570736\n",
            "Iteration 1694, loss = 0.00566767\n",
            "Iteration 1695, loss = 0.00568928\n",
            "Iteration 1696, loss = 0.00569905\n",
            "Iteration 1697, loss = 0.00571422\n",
            "Iteration 1698, loss = 0.00565343\n",
            "Iteration 1699, loss = 0.00565197\n",
            "Iteration 1700, loss = 0.00564707\n",
            "Iteration 1701, loss = 0.00563963\n",
            "Iteration 1702, loss = 0.00562595\n",
            "Iteration 1703, loss = 0.00563754\n",
            "Iteration 1704, loss = 0.00565821\n",
            "Iteration 1705, loss = 0.00566636\n",
            "Iteration 1706, loss = 0.00561795\n",
            "Iteration 1707, loss = 0.00561672\n",
            "Iteration 1708, loss = 0.00558655\n",
            "Iteration 1709, loss = 0.00558663\n",
            "Iteration 1710, loss = 0.00558587\n",
            "Iteration 1711, loss = 0.00556042\n",
            "Iteration 1712, loss = 0.00558811\n",
            "Iteration 1713, loss = 0.00559943\n",
            "Iteration 1714, loss = 0.00557375\n",
            "Iteration 1715, loss = 0.00557016\n",
            "Iteration 1716, loss = 0.00557374\n",
            "Iteration 1717, loss = 0.00560091\n",
            "Iteration 1718, loss = 0.00552973\n",
            "Iteration 1719, loss = 0.00553344\n",
            "Iteration 1720, loss = 0.00552846\n",
            "Iteration 1721, loss = 0.00552317\n",
            "Iteration 1722, loss = 0.00551048\n",
            "Iteration 1723, loss = 0.00553473\n",
            "Iteration 1724, loss = 0.00551384\n",
            "Iteration 1725, loss = 0.00546444\n",
            "Iteration 1726, loss = 0.00555258\n",
            "Iteration 1727, loss = 0.00548615\n",
            "Iteration 1728, loss = 0.00546617\n",
            "Iteration 1729, loss = 0.00548698\n",
            "Iteration 1730, loss = 0.00542426\n",
            "Iteration 1731, loss = 0.00552730\n",
            "Iteration 1732, loss = 0.00544727\n",
            "Iteration 1733, loss = 0.00543984\n",
            "Iteration 1734, loss = 0.00544814\n",
            "Iteration 1735, loss = 0.00542410\n",
            "Iteration 1736, loss = 0.00545739\n",
            "Iteration 1737, loss = 0.00543448\n",
            "Iteration 1738, loss = 0.00548440\n",
            "Iteration 1739, loss = 0.00540861\n",
            "Iteration 1740, loss = 0.00545938\n",
            "Iteration 1741, loss = 0.00546158\n",
            "Iteration 1742, loss = 0.00540870\n",
            "Iteration 1743, loss = 0.00538741\n",
            "Iteration 1744, loss = 0.00538411\n",
            "Iteration 1745, loss = 0.00538495\n",
            "Iteration 1746, loss = 0.00535521\n",
            "Iteration 1747, loss = 0.00534984\n",
            "Iteration 1748, loss = 0.00539176\n",
            "Iteration 1749, loss = 0.00537456\n",
            "Iteration 1750, loss = 0.00537954\n",
            "Iteration 1751, loss = 0.00535365\n",
            "Iteration 1752, loss = 0.00535168\n",
            "Iteration 1753, loss = 0.00535438\n",
            "Iteration 1754, loss = 0.00532732\n",
            "Iteration 1755, loss = 0.00531589\n",
            "Iteration 1756, loss = 0.00529597\n",
            "Iteration 1757, loss = 0.00531535\n",
            "Iteration 1758, loss = 0.00533413\n",
            "Iteration 1759, loss = 0.00529914\n",
            "Iteration 1760, loss = 0.00528511\n",
            "Iteration 1761, loss = 0.00526644\n",
            "Iteration 1762, loss = 0.00527041\n",
            "Iteration 1763, loss = 0.00527747\n",
            "Iteration 1764, loss = 0.00528195\n",
            "Iteration 1765, loss = 0.00526523\n",
            "Iteration 1766, loss = 0.00525624\n",
            "Iteration 1767, loss = 0.00524678\n",
            "Iteration 1768, loss = 0.00523599\n",
            "Iteration 1769, loss = 0.00525769\n",
            "Iteration 1770, loss = 0.00523539\n",
            "Iteration 1771, loss = 0.00527657\n",
            "Iteration 1772, loss = 0.00521429\n",
            "Iteration 1773, loss = 0.00521452\n",
            "Iteration 1774, loss = 0.00521200\n",
            "Iteration 1775, loss = 0.00522736\n",
            "Iteration 1776, loss = 0.00524098\n",
            "Iteration 1777, loss = 0.00522639\n",
            "Iteration 1778, loss = 0.00518321\n",
            "Iteration 1779, loss = 0.00517542\n",
            "Iteration 1780, loss = 0.00518279\n",
            "Iteration 1781, loss = 0.00518845\n",
            "Iteration 1782, loss = 0.00516879\n",
            "Iteration 1783, loss = 0.00516835\n",
            "Iteration 1784, loss = 0.00524633\n",
            "Iteration 1785, loss = 0.00518695\n",
            "Iteration 1786, loss = 0.00515520\n",
            "Iteration 1787, loss = 0.00518014\n",
            "Iteration 1788, loss = 0.00513711\n",
            "Iteration 1789, loss = 0.00517632\n",
            "Iteration 1790, loss = 0.00514884\n",
            "Iteration 1791, loss = 0.00512844\n",
            "Iteration 1792, loss = 0.00511390\n",
            "Iteration 1793, loss = 0.00512694\n",
            "Iteration 1794, loss = 0.00510007\n",
            "Iteration 1795, loss = 0.00512078\n",
            "Iteration 1796, loss = 0.00512883\n",
            "Iteration 1797, loss = 0.00510054\n",
            "Iteration 1798, loss = 0.00511372\n",
            "Iteration 1799, loss = 0.00509364\n",
            "Iteration 1800, loss = 0.00508662\n",
            "Iteration 1801, loss = 0.00509050\n",
            "Iteration 1802, loss = 0.00507915\n",
            "Iteration 1803, loss = 0.00509038\n",
            "Iteration 1804, loss = 0.00504758\n",
            "Iteration 1805, loss = 0.00507353\n",
            "Iteration 1806, loss = 0.00502385\n",
            "Iteration 1807, loss = 0.00508690\n",
            "Iteration 1808, loss = 0.00503181\n",
            "Iteration 1809, loss = 0.00500536\n",
            "Iteration 1810, loss = 0.00505282\n",
            "Iteration 1811, loss = 0.00505384\n",
            "Iteration 1812, loss = 0.00506548\n",
            "Iteration 1813, loss = 0.00503640\n",
            "Iteration 1814, loss = 0.00499747\n",
            "Iteration 1815, loss = 0.00500482\n",
            "Iteration 1816, loss = 0.00502787\n",
            "Iteration 1817, loss = 0.00505755\n",
            "Iteration 1818, loss = 0.00499313\n",
            "Iteration 1819, loss = 0.00500809\n",
            "Iteration 1820, loss = 0.00498045\n",
            "Iteration 1821, loss = 0.00497123\n",
            "Iteration 1822, loss = 0.00496441\n",
            "Iteration 1823, loss = 0.00497734\n",
            "Iteration 1824, loss = 0.00494822\n",
            "Iteration 1825, loss = 0.00495815\n",
            "Iteration 1826, loss = 0.00496879\n",
            "Iteration 1827, loss = 0.00498399\n",
            "Iteration 1828, loss = 0.00494081\n",
            "Iteration 1829, loss = 0.00492815\n",
            "Iteration 1830, loss = 0.00493536\n",
            "Iteration 1831, loss = 0.00492839\n",
            "Iteration 1832, loss = 0.00491881\n",
            "Iteration 1833, loss = 0.00491151\n",
            "Iteration 1834, loss = 0.00491012\n",
            "Iteration 1835, loss = 0.00492065\n",
            "Iteration 1836, loss = 0.00490772\n",
            "Iteration 1837, loss = 0.00491257\n",
            "Iteration 1838, loss = 0.00488967\n",
            "Iteration 1839, loss = 0.00490485\n",
            "Iteration 1840, loss = 0.00490579\n",
            "Iteration 1841, loss = 0.00491559\n",
            "Iteration 1842, loss = 0.00496741\n",
            "Iteration 1843, loss = 0.00486319\n",
            "Iteration 1844, loss = 0.00485912\n",
            "Iteration 1845, loss = 0.00486824\n",
            "Iteration 1846, loss = 0.00492849\n",
            "Iteration 1847, loss = 0.00482326\n",
            "Iteration 1848, loss = 0.00486865\n",
            "Iteration 1849, loss = 0.00488902\n",
            "Iteration 1850, loss = 0.00486682\n",
            "Iteration 1851, loss = 0.00486870\n",
            "Iteration 1852, loss = 0.00484195\n",
            "Iteration 1853, loss = 0.00487592\n",
            "Iteration 1854, loss = 0.00483320\n",
            "Iteration 1855, loss = 0.00482512\n",
            "Iteration 1856, loss = 0.00486194\n",
            "Iteration 1857, loss = 0.00481076\n",
            "Iteration 1858, loss = 0.00480190\n",
            "Iteration 1859, loss = 0.00480324\n",
            "Iteration 1860, loss = 0.00479476\n",
            "Iteration 1861, loss = 0.00478300\n",
            "Iteration 1862, loss = 0.00479473\n",
            "Iteration 1863, loss = 0.00478467\n",
            "Iteration 1864, loss = 0.00481886\n",
            "Iteration 1865, loss = 0.00477192\n",
            "Iteration 1866, loss = 0.00479105\n",
            "Iteration 1867, loss = 0.00479103\n",
            "Iteration 1868, loss = 0.00477156\n",
            "Iteration 1869, loss = 0.00482409\n",
            "Iteration 1870, loss = 0.00476070\n",
            "Iteration 1871, loss = 0.00476730\n",
            "Iteration 1872, loss = 0.00478895\n",
            "Iteration 1873, loss = 0.00473374\n",
            "Iteration 1874, loss = 0.00482857\n",
            "Iteration 1875, loss = 0.00472528\n",
            "Iteration 1876, loss = 0.00473576\n",
            "Iteration 1877, loss = 0.00472425\n",
            "Iteration 1878, loss = 0.00476297\n",
            "Iteration 1879, loss = 0.00476969\n",
            "Iteration 1880, loss = 0.00470343\n",
            "Iteration 1881, loss = 0.00468831\n",
            "Iteration 1882, loss = 0.00470660\n",
            "Iteration 1883, loss = 0.00472341\n",
            "Iteration 1884, loss = 0.00473124\n",
            "Iteration 1885, loss = 0.00466926\n",
            "Iteration 1886, loss = 0.00468167\n",
            "Iteration 1887, loss = 0.00476011\n",
            "Iteration 1888, loss = 0.00469274\n",
            "Iteration 1889, loss = 0.00465954\n",
            "Iteration 1890, loss = 0.00467681\n",
            "Iteration 1891, loss = 0.00466264\n",
            "Iteration 1892, loss = 0.00466383\n",
            "Iteration 1893, loss = 0.00468220\n",
            "Iteration 1894, loss = 0.00466131\n",
            "Iteration 1895, loss = 0.00466372\n",
            "Iteration 1896, loss = 0.00464792\n",
            "Iteration 1897, loss = 0.00463444\n",
            "Iteration 1898, loss = 0.00465642\n",
            "Iteration 1899, loss = 0.00462286\n",
            "Iteration 1900, loss = 0.00462439\n",
            "Iteration 1901, loss = 0.00461636\n",
            "Iteration 1902, loss = 0.00463282\n",
            "Iteration 1903, loss = 0.00461699\n",
            "Iteration 1904, loss = 0.00461558\n",
            "Iteration 1905, loss = 0.00462180\n",
            "Iteration 1906, loss = 0.00462248\n",
            "Iteration 1907, loss = 0.00460840\n",
            "Iteration 1908, loss = 0.00462103\n",
            "Iteration 1909, loss = 0.00462440\n",
            "Iteration 1910, loss = 0.00458033\n",
            "Iteration 1911, loss = 0.00459914\n",
            "Iteration 1912, loss = 0.00458275\n",
            "Iteration 1913, loss = 0.00456105\n",
            "Iteration 1914, loss = 0.00455990\n",
            "Iteration 1915, loss = 0.00459830\n",
            "Iteration 1916, loss = 0.00464055\n",
            "Iteration 1917, loss = 0.00459194\n",
            "Iteration 1918, loss = 0.00455482\n",
            "Iteration 1919, loss = 0.00455179\n",
            "Iteration 1920, loss = 0.00456160\n",
            "Iteration 1921, loss = 0.00454947\n",
            "Iteration 1922, loss = 0.00453294\n",
            "Iteration 1923, loss = 0.00456025\n",
            "Iteration 1924, loss = 0.00453426\n",
            "Iteration 1925, loss = 0.00452426\n",
            "Iteration 1926, loss = 0.00453968\n",
            "Iteration 1927, loss = 0.00454171\n",
            "Iteration 1928, loss = 0.00451796\n",
            "Iteration 1929, loss = 0.00449970\n",
            "Iteration 1930, loss = 0.00451066\n",
            "Iteration 1931, loss = 0.00449212\n",
            "Iteration 1932, loss = 0.00452800\n",
            "Iteration 1933, loss = 0.00451393\n",
            "Iteration 1934, loss = 0.00452630\n",
            "Iteration 1935, loss = 0.00450638\n",
            "Iteration 1936, loss = 0.00448684\n",
            "Iteration 1937, loss = 0.00447595\n",
            "Iteration 1938, loss = 0.00448052\n",
            "Iteration 1939, loss = 0.00448790\n",
            "Iteration 1940, loss = 0.00448192\n",
            "Iteration 1941, loss = 0.00445839\n",
            "Iteration 1942, loss = 0.00445708\n",
            "Iteration 1943, loss = 0.00446625\n",
            "Iteration 1944, loss = 0.00444987\n",
            "Iteration 1945, loss = 0.00445673\n",
            "Iteration 1946, loss = 0.00446199\n",
            "Iteration 1947, loss = 0.00446240\n",
            "Iteration 1948, loss = 0.00443517\n",
            "Iteration 1949, loss = 0.00445111\n",
            "Iteration 1950, loss = 0.00449648\n",
            "Iteration 1951, loss = 0.00447807\n",
            "Iteration 1952, loss = 0.00445132\n",
            "Iteration 1953, loss = 0.00443816\n",
            "Iteration 1954, loss = 0.00444101\n",
            "Iteration 1955, loss = 0.00441363\n",
            "Iteration 1956, loss = 0.00443397\n",
            "Iteration 1957, loss = 0.00438887\n",
            "Iteration 1958, loss = 0.00444834\n",
            "Iteration 1959, loss = 0.00442587\n",
            "Iteration 1960, loss = 0.00440042\n",
            "Iteration 1961, loss = 0.00444105\n",
            "Iteration 1962, loss = 0.00438429\n",
            "Iteration 1963, loss = 0.00438810\n",
            "Iteration 1964, loss = 0.00443514\n",
            "Iteration 1965, loss = 0.00440827\n",
            "Iteration 1966, loss = 0.00439048\n",
            "Iteration 1967, loss = 0.00438521\n",
            "Iteration 1968, loss = 0.00437695\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(hidden_layer_sizes=(2, 2), max_iter=15000, tol=1e-05,\n",
              "              verbose=True)"
            ],
            "text/html": [
              "<style>#sk-container-id-11 {color: black;background-color: white;}#sk-container-id-11 pre{padding: 0;}#sk-container-id-11 div.sk-toggleable {background-color: white;}#sk-container-id-11 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-11 label.sk-toggleable__label-arrow:before {content: \"\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-11 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-11 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-11 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-11 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-11 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-11 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\";}#sk-container-id-11 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-11 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-11 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-11 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-11 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-11 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-11 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-11 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-11 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-11 div.sk-item {position: relative;z-index: 1;}#sk-container-id-11 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-11 div.sk-item::before, #sk-container-id-11 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-11 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-11 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-11 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-11 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-11 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-11 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-11 div.sk-label-container {text-align: center;}#sk-container-id-11 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-11 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-11\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(hidden_layer_sizes=(2, 2), max_iter=15000, tol=1e-05,\n",
              "              verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" checked><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(hidden_layer_sizes=(2, 2), max_iter=15000, tol=1e-05,\n",
              "              verbose=True)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "previsoes = rede_neural_credit.predict(X_credit_teste)\n",
        "previsoes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_5UYnYZiZXeZ",
        "outputId": "eb553784-3cb7-4200-a10d-07dd4b81bd31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
              "       0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,\n",
              "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
              "       0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n",
              "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
              "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_credit_teste"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zuombc5tZivI",
        "outputId": "2e18c467-cdbd-4722-e145-98ad88a16052"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
              "       0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,\n",
              "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
              "       0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n",
              "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
              "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "accuracy_score(y_credit_teste, previsoes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S18vw5LHZn1Z",
        "outputId": "95ea0338-25ff-46be-8d15-5eef2615562c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from yellowbrick.classifier import ConfusionMatrix\n",
        "cm = ConfusionMatrix(rede_neural_credit)\n",
        "cm.fit(X_credit_treinamento, y_credit_treinamento)\n",
        "cm.score(X_credit_teste, y_credit_teste)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        },
        "id": "pWMUrcrqaTJp",
        "outputId": "4a439219-71b1-4f64-a6a4-7f808035b8a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 40
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x550 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_credit_teste, previsoes))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uz3UconubQFq",
        "outputId": "8ce27a6f-2aae-4f2e-cf84-df2e6188b18d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       436\n",
            "           1       1.00      1.00      1.00        64\n",
            "\n",
            "    accuracy                           1.00       500\n",
            "   macro avg       1.00      1.00      1.00       500\n",
            "weighted avg       1.00      1.00      1.00       500\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Base census"
      ],
      "metadata": {
        "id": "1gbBVdufj9BN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/driver/MyDrive/sensus.pkl\", 'rb') as f:\n",
        "  X_census_train, X_census_test, y_census_train, y_census_test = pickle.load(f)"
      ],
      "metadata": {
        "id": "kVePUR3kj_fY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_census_train.shape, y_census_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HuvNe_rhkkEm",
        "outputId": "41341758-8d96-4f2a-a8e5-87db410d324a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((27676, 108), (27676,))"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_census_test.shape, y_census_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ygE-ikTLk1lM",
        "outputId": "303ce1b7-5e1d-4e5f-8f75-3411ccd7715d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((4885, 108), (4885,))"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(108 + 1) / 2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IN9f7X8ElXQy",
        "outputId": "a371e0c8-01fa-4041-fb1d-2a3108d2f85c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "54.5"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rede_neural_cenus = MLPClassifier(verbose=True, max_iter=1500, tol=0.0000010, hidden_layer_sizes=(55,55))\n",
        "\n",
        "rede_neural_cenus.fit(X_census_train, y_census_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "WQlrszPXlFaZ",
        "outputId": "e308f4d1-1fcb-41f8-d2f8-aba4181ff2c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 0.38019710\n",
            "Iteration 2, loss = 0.32303718\n",
            "Iteration 3, loss = 0.31223182\n",
            "Iteration 4, loss = 0.30428646\n",
            "Iteration 5, loss = 0.29983042\n",
            "Iteration 6, loss = 0.29632301\n",
            "Iteration 7, loss = 0.29331494\n",
            "Iteration 8, loss = 0.29043302\n",
            "Iteration 9, loss = 0.28835657\n",
            "Iteration 10, loss = 0.28569758\n",
            "Iteration 11, loss = 0.28346212\n",
            "Iteration 12, loss = 0.28259268\n",
            "Iteration 13, loss = 0.28098760\n",
            "Iteration 14, loss = 0.27816849\n",
            "Iteration 15, loss = 0.27649325\n",
            "Iteration 16, loss = 0.27542356\n",
            "Iteration 17, loss = 0.27371908\n",
            "Iteration 18, loss = 0.27216168\n",
            "Iteration 19, loss = 0.26980618\n",
            "Iteration 20, loss = 0.26904321\n",
            "Iteration 21, loss = 0.26801735\n",
            "Iteration 22, loss = 0.26694017\n",
            "Iteration 23, loss = 0.26497833\n",
            "Iteration 24, loss = 0.26319421\n",
            "Iteration 25, loss = 0.26270122\n",
            "Iteration 26, loss = 0.26072834\n",
            "Iteration 27, loss = 0.25966511\n",
            "Iteration 28, loss = 0.25840678\n",
            "Iteration 29, loss = 0.25665757\n",
            "Iteration 30, loss = 0.25506974\n",
            "Iteration 31, loss = 0.25395652\n",
            "Iteration 32, loss = 0.25299075\n",
            "Iteration 33, loss = 0.25154886\n",
            "Iteration 34, loss = 0.25073816\n",
            "Iteration 35, loss = 0.24890275\n",
            "Iteration 36, loss = 0.24873938\n",
            "Iteration 37, loss = 0.24754217\n",
            "Iteration 38, loss = 0.24600658\n",
            "Iteration 39, loss = 0.24426557\n",
            "Iteration 40, loss = 0.24476376\n",
            "Iteration 41, loss = 0.24334163\n",
            "Iteration 42, loss = 0.24190961\n",
            "Iteration 43, loss = 0.24126757\n",
            "Iteration 44, loss = 0.23930971\n",
            "Iteration 45, loss = 0.23912117\n",
            "Iteration 46, loss = 0.23790032\n",
            "Iteration 47, loss = 0.23699613\n",
            "Iteration 48, loss = 0.23652491\n",
            "Iteration 49, loss = 0.23525252\n",
            "Iteration 50, loss = 0.23363654\n",
            "Iteration 51, loss = 0.23296253\n",
            "Iteration 52, loss = 0.23234161\n",
            "Iteration 53, loss = 0.23136627\n",
            "Iteration 54, loss = 0.23232895\n",
            "Iteration 55, loss = 0.23017570\n",
            "Iteration 56, loss = 0.22988852\n",
            "Iteration 57, loss = 0.22835136\n",
            "Iteration 58, loss = 0.22712397\n",
            "Iteration 59, loss = 0.22684976\n",
            "Iteration 60, loss = 0.22612850\n",
            "Iteration 61, loss = 0.22392207\n",
            "Iteration 62, loss = 0.22414360\n",
            "Iteration 63, loss = 0.22293247\n",
            "Iteration 64, loss = 0.22334049\n",
            "Iteration 65, loss = 0.22151016\n",
            "Iteration 66, loss = 0.22087268\n",
            "Iteration 67, loss = 0.22092117\n",
            "Iteration 68, loss = 0.21991134\n",
            "Iteration 69, loss = 0.21931468\n",
            "Iteration 70, loss = 0.21848204\n",
            "Iteration 71, loss = 0.21829612\n",
            "Iteration 72, loss = 0.21673031\n",
            "Iteration 73, loss = 0.21656300\n",
            "Iteration 74, loss = 0.21642128\n",
            "Iteration 75, loss = 0.21478606\n",
            "Iteration 76, loss = 0.21626410\n",
            "Iteration 77, loss = 0.21311446\n",
            "Iteration 78, loss = 0.21233322\n",
            "Iteration 79, loss = 0.21239492\n",
            "Iteration 80, loss = 0.21193471\n",
            "Iteration 81, loss = 0.21239362\n",
            "Iteration 82, loss = 0.21019227\n",
            "Iteration 83, loss = 0.20909444\n",
            "Iteration 84, loss = 0.20891744\n",
            "Iteration 85, loss = 0.20938895\n",
            "Iteration 86, loss = 0.20750024\n",
            "Iteration 87, loss = 0.20735495\n",
            "Iteration 88, loss = 0.20646540\n",
            "Iteration 89, loss = 0.20636395\n",
            "Iteration 90, loss = 0.20548852\n",
            "Iteration 91, loss = 0.20432633\n",
            "Iteration 92, loss = 0.20471666\n",
            "Iteration 93, loss = 0.20477704\n",
            "Iteration 94, loss = 0.20338923\n",
            "Iteration 95, loss = 0.20388464\n",
            "Iteration 96, loss = 0.20334640\n",
            "Iteration 97, loss = 0.20233511\n",
            "Iteration 98, loss = 0.20149648\n",
            "Iteration 99, loss = 0.20105400\n",
            "Iteration 100, loss = 0.19984222\n",
            "Iteration 101, loss = 0.20031155\n",
            "Iteration 102, loss = 0.19949402\n",
            "Iteration 103, loss = 0.19815273\n",
            "Iteration 104, loss = 0.19789584\n",
            "Iteration 105, loss = 0.19846909\n",
            "Iteration 106, loss = 0.19766150\n",
            "Iteration 107, loss = 0.19689735\n",
            "Iteration 108, loss = 0.19616376\n",
            "Iteration 109, loss = 0.19557415\n",
            "Iteration 110, loss = 0.19594468\n",
            "Iteration 111, loss = 0.19511585\n",
            "Iteration 112, loss = 0.19362305\n",
            "Iteration 113, loss = 0.19363416\n",
            "Iteration 114, loss = 0.19230708\n",
            "Iteration 115, loss = 0.19366637\n",
            "Iteration 116, loss = 0.19253338\n",
            "Iteration 117, loss = 0.19258853\n",
            "Iteration 118, loss = 0.19130843\n",
            "Iteration 119, loss = 0.19025040\n",
            "Iteration 120, loss = 0.18958789\n",
            "Iteration 121, loss = 0.19076971\n",
            "Iteration 122, loss = 0.19108642\n",
            "Iteration 123, loss = 0.18953308\n",
            "Iteration 124, loss = 0.19018552\n",
            "Iteration 125, loss = 0.19011117\n",
            "Iteration 126, loss = 0.18751468\n",
            "Iteration 127, loss = 0.18775741\n",
            "Iteration 128, loss = 0.18811875\n",
            "Iteration 129, loss = 0.18692213\n",
            "Iteration 130, loss = 0.18593007\n",
            "Iteration 131, loss = 0.18600315\n",
            "Iteration 132, loss = 0.18498067\n",
            "Iteration 133, loss = 0.18605027\n",
            "Iteration 134, loss = 0.18528785\n",
            "Iteration 135, loss = 0.18460748\n",
            "Iteration 136, loss = 0.18268347\n",
            "Iteration 137, loss = 0.18357075\n",
            "Iteration 138, loss = 0.18320719\n",
            "Iteration 139, loss = 0.18300673\n",
            "Iteration 140, loss = 0.18215550\n",
            "Iteration 141, loss = 0.18243772\n",
            "Iteration 142, loss = 0.18175534\n",
            "Iteration 143, loss = 0.18152452\n",
            "Iteration 144, loss = 0.18235053\n",
            "Iteration 145, loss = 0.18043491\n",
            "Iteration 146, loss = 0.18154968\n",
            "Iteration 147, loss = 0.18106091\n",
            "Iteration 148, loss = 0.17941838\n",
            "Iteration 149, loss = 0.18004378\n",
            "Iteration 150, loss = 0.17907544\n",
            "Iteration 151, loss = 0.17918549\n",
            "Iteration 152, loss = 0.17954982\n",
            "Iteration 153, loss = 0.17899711\n",
            "Iteration 154, loss = 0.17793176\n",
            "Iteration 155, loss = 0.17699158\n",
            "Iteration 156, loss = 0.17702858\n",
            "Iteration 157, loss = 0.17770421\n",
            "Iteration 158, loss = 0.17673402\n",
            "Iteration 159, loss = 0.17586215\n",
            "Iteration 160, loss = 0.17549773\n",
            "Iteration 161, loss = 0.17653357\n",
            "Iteration 162, loss = 0.17523746\n",
            "Iteration 163, loss = 0.17779757\n",
            "Iteration 164, loss = 0.17476632\n",
            "Iteration 165, loss = 0.17516818\n",
            "Iteration 166, loss = 0.17378566\n",
            "Iteration 167, loss = 0.17557993\n",
            "Iteration 168, loss = 0.17349588\n",
            "Iteration 169, loss = 0.17423390\n",
            "Iteration 170, loss = 0.17466466\n",
            "Iteration 171, loss = 0.17343713\n",
            "Iteration 172, loss = 0.17355378\n",
            "Iteration 173, loss = 0.17311311\n",
            "Iteration 174, loss = 0.17157821\n",
            "Iteration 175, loss = 0.17061108\n",
            "Iteration 176, loss = 0.17262791\n",
            "Iteration 177, loss = 0.17212018\n",
            "Iteration 178, loss = 0.17089561\n",
            "Iteration 179, loss = 0.17187399\n",
            "Iteration 180, loss = 0.17087160\n",
            "Iteration 181, loss = 0.17265205\n",
            "Iteration 182, loss = 0.16976460\n",
            "Iteration 183, loss = 0.17274521\n",
            "Iteration 184, loss = 0.16978101\n",
            "Iteration 185, loss = 0.16965621\n",
            "Iteration 186, loss = 0.16832117\n",
            "Iteration 187, loss = 0.16888011\n",
            "Iteration 188, loss = 0.16718956\n",
            "Iteration 189, loss = 0.16893216\n",
            "Iteration 190, loss = 0.16728430\n",
            "Iteration 191, loss = 0.16781094\n",
            "Iteration 192, loss = 0.16763934\n",
            "Iteration 193, loss = 0.16810854\n",
            "Iteration 194, loss = 0.16720166\n",
            "Iteration 195, loss = 0.16808594\n",
            "Iteration 196, loss = 0.16561871\n",
            "Iteration 197, loss = 0.16730900\n",
            "Iteration 198, loss = 0.16608748\n",
            "Iteration 199, loss = 0.16609475\n",
            "Iteration 200, loss = 0.16589908\n",
            "Iteration 201, loss = 0.16620787\n",
            "Iteration 202, loss = 0.16642452\n",
            "Iteration 203, loss = 0.16504483\n",
            "Iteration 204, loss = 0.16358421\n",
            "Iteration 205, loss = 0.16505740\n",
            "Iteration 206, loss = 0.16348159\n",
            "Iteration 207, loss = 0.16569626\n",
            "Iteration 208, loss = 0.16371640\n",
            "Iteration 209, loss = 0.16456926\n",
            "Iteration 210, loss = 0.16431524\n",
            "Iteration 211, loss = 0.16310155\n",
            "Iteration 212, loss = 0.16250478\n",
            "Iteration 213, loss = 0.16360077\n",
            "Iteration 214, loss = 0.16200726\n",
            "Iteration 215, loss = 0.16211660\n",
            "Iteration 216, loss = 0.16085798\n",
            "Iteration 217, loss = 0.16295637\n",
            "Iteration 218, loss = 0.16139813\n",
            "Iteration 219, loss = 0.16101824\n",
            "Iteration 220, loss = 0.16151464\n",
            "Iteration 221, loss = 0.16163445\n",
            "Iteration 222, loss = 0.16222255\n",
            "Iteration 223, loss = 0.16124825\n",
            "Iteration 224, loss = 0.15991355\n",
            "Iteration 225, loss = 0.16044371\n",
            "Iteration 226, loss = 0.15942682\n",
            "Iteration 227, loss = 0.15956678\n",
            "Iteration 228, loss = 0.15789766\n",
            "Iteration 229, loss = 0.15976013\n",
            "Iteration 230, loss = 0.15844339\n",
            "Iteration 231, loss = 0.15873314\n",
            "Iteration 232, loss = 0.15842794\n",
            "Iteration 233, loss = 0.15788168\n",
            "Iteration 234, loss = 0.15861056\n",
            "Iteration 235, loss = 0.15779243\n",
            "Iteration 236, loss = 0.15994255\n",
            "Iteration 237, loss = 0.15876406\n",
            "Iteration 238, loss = 0.15751872\n",
            "Iteration 239, loss = 0.15907414\n",
            "Iteration 240, loss = 0.15692026\n",
            "Iteration 241, loss = 0.15880319\n",
            "Iteration 242, loss = 0.15500089\n",
            "Iteration 243, loss = 0.15588254\n",
            "Iteration 244, loss = 0.15518777\n",
            "Iteration 245, loss = 0.15539275\n",
            "Iteration 246, loss = 0.15720585\n",
            "Iteration 247, loss = 0.15607326\n",
            "Iteration 248, loss = 0.15611507\n",
            "Iteration 249, loss = 0.15533919\n",
            "Iteration 250, loss = 0.15604258\n",
            "Iteration 251, loss = 0.15485865\n",
            "Iteration 252, loss = 0.15455405\n",
            "Iteration 253, loss = 0.15473664\n",
            "Iteration 254, loss = 0.15436235\n",
            "Iteration 255, loss = 0.15357156\n",
            "Iteration 256, loss = 0.15351152\n",
            "Iteration 257, loss = 0.15497996\n",
            "Iteration 258, loss = 0.15436778\n",
            "Iteration 259, loss = 0.15455921\n",
            "Iteration 260, loss = 0.15218726\n",
            "Iteration 261, loss = 0.15310421\n",
            "Iteration 262, loss = 0.15234520\n",
            "Iteration 263, loss = 0.15534415\n",
            "Iteration 264, loss = 0.15286900\n",
            "Iteration 265, loss = 0.15360282\n",
            "Iteration 266, loss = 0.15104669\n",
            "Iteration 267, loss = 0.15151035\n",
            "Iteration 268, loss = 0.15161017\n",
            "Iteration 269, loss = 0.15361524\n",
            "Iteration 270, loss = 0.15246530\n",
            "Iteration 271, loss = 0.15128832\n",
            "Iteration 272, loss = 0.15116223\n",
            "Iteration 273, loss = 0.15148388\n",
            "Iteration 274, loss = 0.15047224\n",
            "Iteration 275, loss = 0.15063605\n",
            "Iteration 276, loss = 0.14962599\n",
            "Iteration 277, loss = 0.15152987\n",
            "Iteration 278, loss = 0.14984523\n",
            "Iteration 279, loss = 0.15030084\n",
            "Iteration 280, loss = 0.14971416\n",
            "Iteration 281, loss = 0.15063296\n",
            "Iteration 282, loss = 0.14874046\n",
            "Iteration 283, loss = 0.15026549\n",
            "Iteration 284, loss = 0.14969923\n",
            "Iteration 285, loss = 0.15002703\n",
            "Iteration 286, loss = 0.15048269\n",
            "Iteration 287, loss = 0.15081354\n",
            "Iteration 288, loss = 0.14830633\n",
            "Iteration 289, loss = 0.14992571\n",
            "Iteration 290, loss = 0.14906582\n",
            "Iteration 291, loss = 0.14775816\n",
            "Iteration 292, loss = 0.14810761\n",
            "Iteration 293, loss = 0.14811837\n",
            "Iteration 294, loss = 0.14839554\n",
            "Iteration 295, loss = 0.14749246\n",
            "Iteration 296, loss = 0.14685745\n",
            "Iteration 297, loss = 0.14744340\n",
            "Iteration 298, loss = 0.14797919\n",
            "Iteration 299, loss = 0.14599178\n",
            "Iteration 300, loss = 0.14621730\n",
            "Iteration 301, loss = 0.14652572\n",
            "Iteration 302, loss = 0.14600994\n",
            "Iteration 303, loss = 0.14772385\n",
            "Iteration 304, loss = 0.14668432\n",
            "Iteration 305, loss = 0.14687306\n",
            "Iteration 306, loss = 0.14540731\n",
            "Iteration 307, loss = 0.14529972\n",
            "Iteration 308, loss = 0.14574545\n",
            "Iteration 309, loss = 0.14683392\n",
            "Iteration 310, loss = 0.14584405\n",
            "Iteration 311, loss = 0.14594374\n",
            "Iteration 312, loss = 0.14777207\n",
            "Iteration 313, loss = 0.14571088\n",
            "Iteration 314, loss = 0.14636246\n",
            "Iteration 315, loss = 0.14453750\n",
            "Iteration 316, loss = 0.14424297\n",
            "Iteration 317, loss = 0.14477065\n",
            "Iteration 318, loss = 0.14710330\n",
            "Iteration 319, loss = 0.14502918\n",
            "Iteration 320, loss = 0.14499592\n",
            "Iteration 321, loss = 0.14555393\n",
            "Iteration 322, loss = 0.14360660\n",
            "Iteration 323, loss = 0.14726738\n",
            "Iteration 324, loss = 0.14559206\n",
            "Iteration 325, loss = 0.14368151\n",
            "Iteration 326, loss = 0.14311611\n",
            "Iteration 327, loss = 0.14270496\n",
            "Iteration 328, loss = 0.14241477\n",
            "Iteration 329, loss = 0.14228985\n",
            "Iteration 330, loss = 0.14542010\n",
            "Iteration 331, loss = 0.14291164\n",
            "Iteration 332, loss = 0.14390267\n",
            "Iteration 333, loss = 0.14447848\n",
            "Iteration 334, loss = 0.14254025\n",
            "Iteration 335, loss = 0.14219909\n",
            "Iteration 336, loss = 0.14190570\n",
            "Iteration 337, loss = 0.14357891\n",
            "Iteration 338, loss = 0.14263084\n",
            "Iteration 339, loss = 0.14271626\n",
            "Iteration 340, loss = 0.14166881\n",
            "Iteration 341, loss = 0.14165347\n",
            "Iteration 342, loss = 0.14234461\n",
            "Iteration 343, loss = 0.14111343\n",
            "Iteration 344, loss = 0.14067674\n",
            "Iteration 345, loss = 0.14303501\n",
            "Iteration 346, loss = 0.14045684\n",
            "Iteration 347, loss = 0.13998229\n",
            "Iteration 348, loss = 0.14171475\n",
            "Iteration 349, loss = 0.14192416\n",
            "Iteration 350, loss = 0.14095720\n",
            "Iteration 351, loss = 0.14258550\n",
            "Iteration 352, loss = 0.14081169\n",
            "Iteration 353, loss = 0.13948397\n",
            "Iteration 354, loss = 0.14040131\n",
            "Iteration 355, loss = 0.14141322\n",
            "Iteration 356, loss = 0.13914977\n",
            "Iteration 357, loss = 0.13998959\n",
            "Iteration 358, loss = 0.13961915\n",
            "Iteration 359, loss = 0.14219750\n",
            "Iteration 360, loss = 0.14136033\n",
            "Iteration 361, loss = 0.14233190\n",
            "Iteration 362, loss = 0.13833724\n",
            "Iteration 363, loss = 0.13995230\n",
            "Iteration 364, loss = 0.13934938\n",
            "Iteration 365, loss = 0.13977536\n",
            "Iteration 366, loss = 0.13937784\n",
            "Iteration 367, loss = 0.14097049\n",
            "Iteration 368, loss = 0.13952528\n",
            "Iteration 369, loss = 0.13840287\n",
            "Iteration 370, loss = 0.13995569\n",
            "Iteration 371, loss = 0.13826443\n",
            "Iteration 372, loss = 0.13830067\n",
            "Iteration 373, loss = 0.13734580\n",
            "Iteration 374, loss = 0.13768030\n",
            "Iteration 375, loss = 0.13693935\n",
            "Iteration 376, loss = 0.13959248\n",
            "Iteration 377, loss = 0.13733733\n",
            "Iteration 378, loss = 0.13824774\n",
            "Iteration 379, loss = 0.13689612\n",
            "Iteration 380, loss = 0.13816339\n",
            "Iteration 381, loss = 0.13852892\n",
            "Iteration 382, loss = 0.13798836\n",
            "Iteration 383, loss = 0.13814343\n",
            "Iteration 384, loss = 0.13795231\n",
            "Iteration 385, loss = 0.13957361\n",
            "Iteration 386, loss = 0.13822905\n",
            "Iteration 387, loss = 0.13932937\n",
            "Iteration 388, loss = 0.14095973\n",
            "Iteration 389, loss = 0.13684840\n",
            "Iteration 390, loss = 0.13558239\n",
            "Iteration 391, loss = 0.13671619\n",
            "Iteration 392, loss = 0.13949978\n",
            "Iteration 393, loss = 0.13749689\n",
            "Iteration 394, loss = 0.13688097\n",
            "Iteration 395, loss = 0.13546758\n",
            "Iteration 396, loss = 0.13594337\n",
            "Iteration 397, loss = 0.13612520\n",
            "Iteration 398, loss = 0.13811681\n",
            "Iteration 399, loss = 0.13769587\n",
            "Iteration 400, loss = 0.13526539\n",
            "Iteration 401, loss = 0.13526768\n",
            "Iteration 402, loss = 0.13433068\n",
            "Iteration 403, loss = 0.13602209\n",
            "Iteration 404, loss = 0.13527737\n",
            "Iteration 405, loss = 0.13630025\n",
            "Iteration 406, loss = 0.13635147\n",
            "Iteration 407, loss = 0.13555747\n",
            "Iteration 408, loss = 0.13624048\n",
            "Iteration 409, loss = 0.13617396\n",
            "Iteration 410, loss = 0.13546658\n",
            "Iteration 411, loss = 0.13622407\n",
            "Iteration 412, loss = 0.13365260\n",
            "Iteration 413, loss = 0.13354479\n",
            "Iteration 414, loss = 0.13643240\n",
            "Iteration 415, loss = 0.13446884\n",
            "Iteration 416, loss = 0.13812618\n",
            "Iteration 417, loss = 0.13813237\n",
            "Iteration 418, loss = 0.13675342\n",
            "Iteration 419, loss = 0.13655418\n",
            "Iteration 420, loss = 0.13332669\n",
            "Iteration 421, loss = 0.13474423\n",
            "Iteration 422, loss = 0.13539506\n",
            "Iteration 423, loss = 0.13438738\n",
            "Iteration 424, loss = 0.13331941\n",
            "Iteration 425, loss = 0.13387543\n",
            "Iteration 426, loss = 0.13340311\n",
            "Iteration 427, loss = 0.13534091\n",
            "Iteration 428, loss = 0.13378215\n",
            "Iteration 429, loss = 0.13303338\n",
            "Iteration 430, loss = 0.13541322\n",
            "Iteration 431, loss = 0.13414074\n",
            "Iteration 432, loss = 0.13322577\n",
            "Iteration 433, loss = 0.13252091\n",
            "Iteration 434, loss = 0.13311351\n",
            "Iteration 435, loss = 0.13238142\n",
            "Iteration 436, loss = 0.13189666\n",
            "Iteration 437, loss = 0.13285836\n",
            "Iteration 438, loss = 0.13465033\n",
            "Iteration 439, loss = 0.13182744\n",
            "Iteration 440, loss = 0.13223451\n",
            "Iteration 441, loss = 0.13336264\n",
            "Iteration 442, loss = 0.13202160\n",
            "Iteration 443, loss = 0.13226198\n",
            "Iteration 444, loss = 0.13357620\n",
            "Iteration 445, loss = 0.13273465\n",
            "Iteration 446, loss = 0.13358744\n",
            "Iteration 447, loss = 0.13220191\n",
            "Iteration 448, loss = 0.13144788\n",
            "Iteration 449, loss = 0.13192997\n",
            "Iteration 450, loss = 0.13149143\n",
            "Iteration 451, loss = 0.13107941\n",
            "Iteration 452, loss = 0.13225350\n",
            "Iteration 453, loss = 0.12918026\n",
            "Iteration 454, loss = 0.12991965\n",
            "Iteration 455, loss = 0.13033037\n",
            "Iteration 456, loss = 0.13280992\n",
            "Iteration 457, loss = 0.13544826\n",
            "Iteration 458, loss = 0.13677241\n",
            "Iteration 459, loss = 0.13225908\n",
            "Iteration 460, loss = 0.13278536\n",
            "Iteration 461, loss = 0.13060096\n",
            "Iteration 462, loss = 0.13233749\n",
            "Iteration 463, loss = 0.13378129\n",
            "Iteration 464, loss = 0.13077547\n",
            "Training loss did not improve more than tol=0.000001 for 10 consecutive epochs. Stopping.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(hidden_layer_sizes=(55, 55), max_iter=1500, tol=1e-06,\n",
              "              verbose=True)"
            ],
            "text/html": [
              "<style>#sk-container-id-13 {color: black;background-color: white;}#sk-container-id-13 pre{padding: 0;}#sk-container-id-13 div.sk-toggleable {background-color: white;}#sk-container-id-13 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-13 label.sk-toggleable__label-arrow:before {content: \"\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-13 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-13 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-13 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-13 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-13 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-13 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\";}#sk-container-id-13 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-13 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-13 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-13 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-13 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-13 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-13 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-13 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-13 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-13 div.sk-item {position: relative;z-index: 1;}#sk-container-id-13 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-13 div.sk-item::before, #sk-container-id-13 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-13 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-13 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-13 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-13 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-13 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-13 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-13 div.sk-label-container {text-align: center;}#sk-container-id-13 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-13 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-13\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(hidden_layer_sizes=(55, 55), max_iter=1500, tol=1e-06,\n",
              "              verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-13\" type=\"checkbox\" checked><label for=\"sk-estimator-id-13\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(hidden_layer_sizes=(55, 55), max_iter=1500, tol=1e-06,\n",
              "              verbose=True)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "previsoes = rede_neural_cenus.predict(X_census_test)\n",
        "previsoes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4NI8_OmduBZe",
        "outputId": "c243c8a7-5be2-4707-e20f-70603aa3af56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([' <=50K', ' <=50K', ' <=50K', ..., ' <=50K', ' <=50K', ' >50K'],\n",
              "      dtype='<U6')"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_census_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JzPNkhjxuNjw",
        "outputId": "ee7fef07-dcdf-4e70-c136-fbfee6cf28c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([' <=50K', ' <=50K', ' <=50K', ..., ' <=50K', ' <=50K', ' <=50K'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "accuracy_score(y_census_test, previsoes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "obgQ4Dx6uThP",
        "outputId": "46e3b799-8967-494d-c099-9d17eac1304f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8149437052200614"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from yellowbrick.classifier import ConfusionMatrix\n",
        "cm = ConfusionMatrix(rede_neural_cenus)\n",
        "cm.fit(X_census_train, y_census_train)\n",
        "cm.score(X_census_test, y_census_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 540
        },
        "id": "FmEpHSsvuzcX",
        "outputId": "f33c30b1-313f-439c-f740-ae925caeb0c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8149437052200614"
            ]
          },
          "metadata": {},
          "execution_count": 51
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x550 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_census_test, previsoes))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HsqTBSnOvNfl",
        "outputId": "51761f05-ee52-41ac-8924-057e25d5775f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "       <=50K       0.87      0.89      0.88      3693\n",
            "        >50K       0.63      0.59      0.61      1192\n",
            "\n",
            "    accuracy                           0.81      4885\n",
            "   macro avg       0.75      0.74      0.74      4885\n",
            "weighted avg       0.81      0.81      0.81      4885\n",
            "\n"
          ]
        }
      ]
    }
  ]
}